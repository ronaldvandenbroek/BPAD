{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..\\..')\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "from utils.fs import RESULTS_RAW_DIR\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subfolders_or_zip_files(experiment_name):\n",
    "    experiment_path = os.path.join(RESULTS_RAW_DIR, experiment_name)\n",
    "    run_names = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name)) or (name.endswith('.zip') and os.path.isfile(os.path.join(experiment_path, name)))]\n",
    "    return run_names\n",
    "\n",
    "def get_temp_folder_path(directory, run_name):\n",
    "    return os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "\n",
    "def unzip_results(directory, run_name):\n",
    "    if run_name.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "        if zipfile.is_zipfile(zip_path):\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                new_run_name = os.path.splitext(run_name)[0]\n",
    "                zip_ref.extractall(get_temp_folder_path(directory, new_run_name))\n",
    "            return new_run_name, True\n",
    "    else:\n",
    "        return run_name, False\n",
    "    \n",
    "def cleanup_temp_folders(directory, run_name):\n",
    "    temp_path = get_temp_folder_path(directory, run_name)\n",
    "    if os.path.exists(temp_path):\n",
    "        shutil.rmtree(temp_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(run_name, verbose=False, directory=None):\n",
    "    if directory:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "    else:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, run_name)\n",
    "\n",
    "    np_files = [file for file in os.listdir(run_path) if file.endswith('.npy') or file.endswith('.npz')]\n",
    "\n",
    "    loaded_data = {}\n",
    "\n",
    "    # Load each .npy or .npz file and use the file name (without extension) as the key\n",
    "    for np_file in np_files:\n",
    "        file_path = os.path.join(run_path, np_file)\n",
    "        key = os.path.splitext(np_file)[0]  # Get the file name without .npy or .npz extension\n",
    "\n",
    "        if np_file.endswith('.npy'):\n",
    "            # Directly load .npy files\n",
    "            loaded_data[key] = np.load(file_path)\n",
    "        elif np_file.endswith('.npz'):\n",
    "            # Safely load .npz files and close the file afterward\n",
    "            with np.load(file_path) as data:\n",
    "                if len(data.files) == 1:\n",
    "                    loaded_data[key] = data[data.files[0]]  # Extract the single array\n",
    "                else:\n",
    "                    raise ValueError(f\"Multiple arrays in .npz file: {file_path}. Expected only one.\")\n",
    "\n",
    "        if verbose: print(f\"{loaded_data[key]} \\t {key}\")\n",
    "\n",
    "    return loaded_data\n",
    "\n",
    "def load_config(run_name, directory=None):\n",
    "    if directory:\n",
    "        config_path = os.path.join(RESULTS_RAW_DIR, directory, run_name, \"config.json\")\n",
    "    else:\n",
    "        config_path = os.path.join(RESULTS_RAW_DIR, run_name, \"config.json\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def get_buckets(keys):\n",
    "    buckets = set()\n",
    "    for key in keys:\n",
    "        numbers = re.findall(r'\\d+', key)\n",
    "        buckets.update(map(int, numbers))\n",
    "    if len(buckets) > 0:\n",
    "        return sorted(buckets)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results, labels, directory, run_name, perspective, level, bucket=None, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    def scatter_plot(ax, results, labels):\n",
    "        y_values = results\n",
    "        x_values = np.arange(len(results))\n",
    "        ax.scatter(x_values[labels == 0], y_values[labels == 0], c='grey', s=3, label='Normal Prefixes', zorder=1)\n",
    "        ax.scatter(x_values[labels == 1], y_values[labels == 1], c='red', s=3, label='Anomalous Prefixes', zorder=2)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Normalize results\n",
    "    results = np.interp(results, (results.min(), results.max()), (0, 1))\n",
    "\n",
    "    subtitle = f'{directory}     {run_name}'\n",
    "    if len(results) == 0:\n",
    "        print(f'ERROR no results found for {subtitle}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        labels = labels[:, perspective]\n",
    "        scatter_plot(ax, results, labels)\n",
    "        \n",
    "        perspective_name = Perspective.values()[perspective]\n",
    "\n",
    "        bucket_string = ''\n",
    "        if bucket is not None:\n",
    "            bucket_string = f'with bucket size {str(bucket)}'\n",
    "        \n",
    "        title = f'Error per Prefix on the {perspective_name} perspective at {level} level {bucket_string}'\n",
    "        \n",
    "        # Print to keep track of plotting\n",
    "        # print(f'\\t {title}')\n",
    "        \n",
    "        plt.title(f'{title}\\n{subtitle}')\n",
    "        plt.xlabel('Prefix Index')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        if zoom:\n",
    "            axins = inset_axes(ax, width=\"60%\", height=\"60%\", loc='upper right')\n",
    "\n",
    "            scatter_plot(axins, results, labels)\n",
    "            axins.set_xlim(zoom[0])\n",
    "            axins.set_ylim(zoom[1])\n",
    "            _,_ = ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=3)\n",
    "\n",
    "        plt.xlabel('Case Index')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        plot_path = f\"plots\\{directory}\\{run_name} \"\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        plt.savefig(f\"{plot_path}\\error_plots\\{perspective_name}_{level}_{bucket_string}.png\", format='png', dpi=300)\n",
    "        \n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def bucket_plot_losses(results_name, labels_name, run_name, directory, bucket_lengths, results, perspective, level, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True, pbar=None):\n",
    "    if bucket_lengths is None:\n",
    "        plot_losses(\n",
    "            results=results[f'{results_name}'], \n",
    "            labels=results[f'{labels_name}'],\n",
    "            directory=directory,\n",
    "            run_name=run_name, perspective=perspective, level=level, bucket=None, zoom=zoom, show_plots=show_plots)\n",
    "        if pbar:\n",
    "            pbar.update(1)       \n",
    "    else:\n",
    "        for bucket in bucket_lengths:\n",
    "            plot_losses(\n",
    "                results=results[f'{results_name}_{bucket}'], \n",
    "                labels=results[f'{labels_name}_{bucket}'],\n",
    "                directory=directory,\n",
    "                run_name=run_name, perspective=perspective, level=level, bucket=bucket, zoom=zoom, show_plots=show_plots)\n",
    "            if pbar:\n",
    "                pbar.update(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes_by_value(arr):\n",
    "    value_to_indexes = {}\n",
    "    for index, value in enumerate(arr):\n",
    "        if value not in value_to_indexes:\n",
    "            value_to_indexes[value] = []\n",
    "        value_to_indexes[value].append(index)\n",
    "    return value_to_indexes\n",
    "\n",
    "def normalize(array):\n",
    "    array = np.array(array)\n",
    "    return np.interp(array, (array.min(), array.max()), (0, 1))\n",
    "\n",
    "# Function to extract the number after the last underscore\n",
    "def extract_number(key):\n",
    "    try:\n",
    "        return int(key.split('_')[-1])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attribute_labels(output, values, case_length, perspective, perspective_label_indices):\n",
    "    # print(values.shape)\n",
    "\n",
    "    perspective_value = values[perspective, :, :, :]\n",
    "    # print(perspective_value.shape)\n",
    "\n",
    "    perspective_masked = perspective_value[:, :case_length, :]\n",
    "    # print(perspective_masked.shape)\n",
    "\n",
    "    perspective_indexed = perspective_masked[:,:,perspective_label_indices[perspective]]\n",
    "    # print(perspective_indexed.shape)\n",
    "\n",
    "    perspective_attribute_value = perspective_indexed.reshape(-1) # Flatten the output\n",
    "    # print(perspective_attribute_value.shape)\n",
    "\n",
    "    output.append(perspective_attribute_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data_for_scoring(results, perspective_label_indices):\n",
    "    labels_DAE_attribute_Arrival_Time = []\n",
    "    labels_DAE_attribute_Workload = []\n",
    "    labels_DAE_attribute_Order = []\n",
    "    labels_DAE_attribute_Attribute  = []\n",
    "\n",
    "    labels_DAE_event = []\n",
    "    labels_DAE_trace = []\n",
    "\n",
    "    result_DAE_attribute_Arrival_Time = []\n",
    "    result_DAE_event_Arrival_Time = []\n",
    "    result_DAE_trace_Arrival_Time = []\n",
    "    result_DAE_attribute_Workload = []\n",
    "    result_DAE_event_Workload = []\n",
    "    result_DAE_trace_Workload = []\n",
    "    result_DAE_attribute_Order = []\n",
    "    result_DAE_event_Order = []\n",
    "    result_DAE_trace_Order = []\n",
    "    result_DAE_attribute_Attribute = []\n",
    "    result_DAE_event_Attribute = []\n",
    "    result_DAE_trace_Attribute = []\n",
    "\n",
    "    for (key, value) in results.items():\n",
    "        # print(key, value.shape)\n",
    "        try:\n",
    "            length = int(key.split('_')[-1])\n",
    "            perspective = key.split('_')[-2]\n",
    "        except: # If it fails it mean there is no bucket, then the length of every trace is max length\n",
    "            if 'attribute' in key or 'event' in key:\n",
    "                length = value.shape[1]\n",
    "            perspective = key.split('_')[-1]\n",
    "        \n",
    "        if 'losses' in key:\n",
    "            continue\n",
    "        elif 'labels' in key:\n",
    "            if 'attribute' in key:\n",
    "                transposed_value = np.transpose(value, (3,0,1,2))# [:, :, :length, :]\n",
    "\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Arrival_Time,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ARRIVAL_TIME,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Attribute,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ATTRIBUTE,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Order,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ORDER,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Workload,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.WORKLOAD,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "\n",
    "                # # print(perspective_value.shape)\n",
    "                # perspective_value = perspective_value.reshape(perspective_value.shape[0], -1)\n",
    "                # # print(perspective_value.shape)\n",
    "                # labels_DAE_attribute.append(perspective_value)\n",
    "            elif 'event' in key:\n",
    "                perspective_value = np.transpose(value, (2,0,1))[:, :, :length]\n",
    "                perspective_value = perspective_value.reshape(perspective_value.shape[0], -1)\n",
    "                labels_DAE_event.append(perspective_value)\n",
    "            elif 'trace' in key:\n",
    "                perspective_value = np.transpose(value, (1,0))\n",
    "                # print(perspective_value.shape)\n",
    "                labels_DAE_trace.append(perspective_value)\n",
    "        elif 'result' in key:\n",
    "            if 'attribute' in key:\n",
    "                # print(value.shape)\n",
    "                # value_max = np.max(value, axis=2)\n",
    "                # print(value.shape, normalize(value.reshape(-1)).shape, perspective)\n",
    "                # print(value.shape)\n",
    "                value = normalize(value.reshape(-1))\n",
    "                # print(value.shape)\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_attribute_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_attribute_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_attribute_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_attribute_Attribute.append(value)\n",
    "            if 'event' in key:\n",
    "                value = normalize(value.reshape(-1))\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_event_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_event_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_event_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_event_Attribute.append(value)\n",
    "            elif 'trace' in key:\n",
    "                value = normalize(value)\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_trace_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_trace_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_trace_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_trace_Attribute.append(value)\n",
    "\n",
    "\n",
    "    # labels_DAE_attribute = np.concatenate(labels_DAE_attribute, axis=1)\n",
    "    labels_DAE_event = np.concatenate(labels_DAE_event, axis=1)    \n",
    "    labels_DAE_trace = np.concatenate(labels_DAE_trace, axis=1)\n",
    "\n",
    "    # print(labels_DAE_attribute.shape)\n",
    "\n",
    "    # print(np.concatenate(result_DAE_event_Order, axis=0).shape)\n",
    "    # print(result_DAE_attribute_Attribute.shape)\n",
    "    # print(result_DAE_attribute_Arrival_Time.shape)\n",
    "    # print(result_DAE_attribute_Workload.shape)\n",
    "\n",
    "    labels_DAE_attribute = [\n",
    "        np.concatenate(labels_DAE_attribute_Order, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Attribute, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Arrival_Time, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_attribute = [\n",
    "        np.concatenate(result_DAE_attribute_Order, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_event = [\n",
    "        np.concatenate(result_DAE_event_Order, axis=0),\n",
    "        np.concatenate(result_DAE_event_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_event_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_event_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_trace = [\n",
    "        np.concatenate(result_DAE_trace_Order, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    return labels_DAE_attribute, labels_DAE_event, labels_DAE_trace, result_DAE_attribute, result_DAE_event, result_DAE_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(precision, recall):\n",
    "    # Check if both precision and recall are zero to avoid division by zero\n",
    "    precision[precision == 0] = 1e-6\n",
    "    recall[recall == 0] = 1e-6\n",
    "\n",
    "    if np.any(precision != 0) and np.any(recall != 0):\n",
    "        return (2 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_scores(y_trues, pred_probs, perspective):\n",
    "    y_true = y_trues[perspective][:]\n",
    "    pred_prob = pred_probs[perspective][:]\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, pred_prob)\n",
    "\n",
    "    # PR-AUC\n",
    "    pr_auc = average_precision_score(y_true, pred_prob)\n",
    "\n",
    "    # F1-Score\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true=y_true, probas_pred=pred_prob)\n",
    "    # print(precision.shape)\n",
    "    # print(recall.shape)\n",
    "    f1s=calculate_f1(precision,recall)\n",
    "    f1s[np.isnan(f1s)] = 0\n",
    "    # print(f1s)\n",
    "    f1_best_index=np.argmax(f1s)\n",
    "    # recall_best_index=np.mean(recall)\n",
    "    # precision_best_index=np.mean(precision)\n",
    "\n",
    "    return roc_auc, pr_auc, f1s[f1_best_index], np.mean(precision), np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(run):\n",
    "    results = run['results']\n",
    "    config = run['config']\n",
    "    timestamp = run['timestamp']\n",
    "    index = run['index']\n",
    "\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda x: extract_number(x[0])))\n",
    "    perspective_label_indices = get_indexes_by_value(config['attribute_perspectives_original'])\n",
    "\n",
    "    (\n",
    "        labels_DAE_attribute, \n",
    "        labels_DAE_event, \n",
    "        labels_DAE_trace, \n",
    "        result_DAE_attribute, \n",
    "        result_DAE_event, \n",
    "        result_DAE_trace\n",
    "    ) = reshape_data_for_scoring(results=sorted_results, perspective_label_indices=perspective_label_indices)\n",
    "\n",
    "    level = ['trace', 'event', 'attribute']\n",
    "    datasets = [labels_DAE_trace, labels_DAE_event, labels_DAE_attribute]\n",
    "    results = [result_DAE_trace, result_DAE_event, result_DAE_attribute]\n",
    "    perspectives = Perspective.keys()\n",
    "\n",
    "    scores = []\n",
    "    for (level, dataset, result), perspective in itertools.product(zip(level, datasets, results), perspectives):\n",
    "        try:\n",
    "            roc_auc, pr_auc, f1, precision, recall = calculate_scores(dataset, result, perspective)\n",
    "            # print(level, perspective, roc_auc, pr_auc)\n",
    "\n",
    "            scores.append({\n",
    "                # High level differentiatiors\n",
    "                'run_name':config['run_name'],\n",
    "                'model':config['model'],\n",
    "                'dataset':config['dataset'],\n",
    "                'timestamp':timestamp,\n",
    "                'index':index,\n",
    "                # 'repeat':config['repeat'],\n",
    "                # Level/Perspectives\n",
    "                'level': level,\n",
    "                'perspective': Perspective.values()[perspective],\n",
    "                # Scores\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'f1': f1,\n",
    "                'precision':precision,\n",
    "                'recall':recall,\n",
    "                'run_time': config['run_time'],\n",
    "                # Config\n",
    "                'batch_size':config['batch_size'],\n",
    "                'prefix':config['prefix'],\n",
    "                'buckets':config['bucket_boundaries'],\n",
    "                'categorical_encoding':config['categorical_encoding'],\n",
    "                'numerical_encoding':config['numerical_encoding'],\n",
    "                'vector_size':config['vector_size'],\n",
    "                'window_size':config['window_size'] \n",
    "            })\n",
    "        except:\n",
    "            print(level, perspective)\n",
    "\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_score_dataframe(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        loaded_df = pd.read_pickle(file_path)\n",
    "        print(\"DataFrame loaded successfully!\")\n",
    "        return loaded_df\n",
    "    else:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Experiment_Real_World_Debug',\n",
    "\n",
    "directories = [\n",
    "    # 'Experiment_Prefix_v2',\n",
    "    # 'Experiment_Batch_Size',\n",
    "    # 'Experiment_Anomaly_Percentage_v2',\n",
    "    # 'Experiment_Synthetic_Dataset_v4',\n",
    "\n",
    "    # 'Experiment_Finetuning_Fixed_Vector_Vector_Sizes',\n",
    "    # 'Experiment_Finetuning_T2V_Window_Vector_Sizes',\n",
    "    # 'Experiment_Finetuning_W2V_Window_Vector_Sizes',\n",
    "\n",
    "    # 'Experiment_Synthetic_All_Models',\n",
    "    'Experiment_Real_World_All_Models',\n",
    "\n",
    "    ] \n",
    "\n",
    "filter_beginning_percentage = 0\n",
    "\n",
    "recalculate = True\n",
    "\n",
    "score_results = True\n",
    "score_summary = True\n",
    "\n",
    "rank_encoders = True\n",
    "\n",
    "plot_results = False\n",
    "show_plots = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = directories[-1]\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = list_subfolders_or_zip_files(directory)\n",
    "print(run_list)\n",
    "print(run_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_path = f\"plots\\{directory}\\\\\"\n",
    "score_file = f\"scores_raw_df.pkl\"\n",
    "all_scores_df = None\n",
    "\n",
    "# Check if scores_raw_df.pkl exists and if yes skip reloading all data, force recalculation if set to true\n",
    "if not recalculate:\n",
    "    all_scores_df = load_score_dataframe(score_path + score_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "scores_dfs = []\n",
    "total_runtime = 0\n",
    "\n",
    "# If no scores_raw_df.pkl exists, recalulate is true or plotting individual runs start loading all data from raw\n",
    "if all_scores_df is None or recalculate is True or plot_results is True:\n",
    "    for index, run_name in enumerate(tqdm(run_list)):\n",
    "        try:\n",
    "            # If needed unzip the data\n",
    "            run_name, from_zip = unzip_results(directory, run_name)\n",
    "\n",
    "            # Loading the data\n",
    "            results = load_results(run_name=run_name, directory=directory)\n",
    "            config = load_config(run_name=run_name, directory=directory)\n",
    "            buckets = get_buckets(results.keys())\n",
    "            timestamp = run_name.split('_')[0]\n",
    "\n",
    "            total_runtime += config['run_time']\n",
    "\n",
    "            # If needed clean up temp folder\n",
    "            if from_zip:\n",
    "                cleanup_temp_folders(directory, run_name)\n",
    "            \n",
    "            # If set filter the first % of results from the run to allow the scoring some grace period\n",
    "            if filter_beginning_percentage != 0:\n",
    "                for key, value in results.items():\n",
    "                    filter_index = int(value.shape[0] / filter_beginning_percentage)\n",
    "                    # print(filter_index)\n",
    "                    results[key] = value[filter_index:]\n",
    "\n",
    "            run = {\n",
    "                \"name\": run_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"results\": results,\n",
    "                \"config\": config,\n",
    "                \"buckets\": buckets,\n",
    "                \"index\": index,\n",
    "            }\n",
    "\n",
    "            # If no preloaded scores exist  \n",
    "            if (all_scores_df is None or recalculate is True) and (score_results is True or rank_encoders is True): \n",
    "                scores_df = score(run=run)\n",
    "                scores_dfs.append(scores_df)\n",
    "\n",
    "            # Only save to runs if plotting results otherwise it is wasting memory\n",
    "            if plot_results:\n",
    "                runs.append(run)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load: {run_name}\")\n",
    "            print(e)\n",
    "\n",
    "# Save the scores dataframe to disk if it has calculated\n",
    "if len(scores_dfs) != 0:\n",
    "    all_scores_df = pd.concat(scores_dfs, ignore_index=True)\n",
    "    os.makedirs(score_path, exist_ok=True)\n",
    "    all_scores_df.to_pickle(score_path + score_file)\n",
    "\n",
    "\n",
    "print(len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_runtime != 0:\n",
    "    output = f\"Total runtime (multiple scales): \\n{round(total_runtime / 3600, 2)} hours \\n{round(total_runtime / 60, 2)} minutes \\n{round(total_runtime, 2)} seconds\"\n",
    "\n",
    "    plot_path = f\"plots\\\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    file_path = f\"{plot_path}\\\\total_runtime.txt\"\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_results:\n",
    "    results_config = [\n",
    "        ('result_DAE_trace_Order', 'labels_DAE_trace', Perspective.ORDER, 'trace'),\n",
    "        ('result_DAE_trace_Attribute', 'labels_DAE_trace', Perspective.ATTRIBUTE, 'trace'),\n",
    "        ('result_DAE_trace_Arrival Time', 'labels_DAE_trace', Perspective.ARRIVAL_TIME, 'trace'),\n",
    "        ('result_DAE_trace_Workload', 'labels_DAE_trace', Perspective.WORKLOAD, 'trace'),\n",
    "    ]\n",
    "\n",
    "    nr_buckets = 0\n",
    "    for run in runs:\n",
    "        if run[\"buckets\"] is None:\n",
    "            nr_buckets += 1\n",
    "        else:\n",
    "            nr_buckets += len(run[\"buckets\"])\n",
    "\n",
    "    total_iterations = nr_buckets * len(results_config)\n",
    "    with tqdm(total=total_iterations, desc=\"Generating Plots\") as pbar:\n",
    "        for run in runs:\n",
    "            # print(f\"Generating: {directory}\\t{run_name}\")\n",
    "            for config in results_config:\n",
    "                # try:\n",
    "                bucket_plot_losses(\n",
    "                    results_name=config[0], \n",
    "                    labels_name=config[1],\n",
    "                    directory=directory,\n",
    "                    run_name=run[\"name\"],\n",
    "                    bucket_lengths=run[\"buckets\"],\n",
    "                    results=run[\"results\"],\n",
    "                    perspective=config[2],\n",
    "                    level=config[3],\n",
    "                    zoom=None,\n",
    "                    show_plots=show_plots,\n",
    "                    pbar=pbar)\n",
    "                # except:\n",
    "                #     print(\"Error loading \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df = all_scores_df.drop(columns=[\"run_name\", \"timestamp\", \"index\", \"numerical_encoding\"])\n",
    "all_scores_df[\"buckets\"] = all_scores_df[\"buckets\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the encoding methods combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"perspective\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "\n",
    "averages_methods = (\n",
    "    all_scores_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        \"roc_auc\": \"mean\", \n",
    "        \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", #[\"mean\", \"std\"],\n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\", #[\"mean\", \"std\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_methods.columns = ['_'.join(col).strip('_') for col in averages_methods.columns]\n",
    "averages_methods = averages_methods.reset_index()\n",
    "\n",
    "averages_methods[\"categorical_encoding\"] = \"All\"\n",
    "# averages_methods[\"timestamp\"] = \"Average\"\n",
    "\n",
    "result_methods_df = pd.concat([all_scores_df, averages_methods], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_methods.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scores_df.shape)\n",
    "print(averages_methods.shape)\n",
    "print(result_methods_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the perspectives combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"categorical_encoding\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "averages_perspective = (\n",
    "    result_methods_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        \"roc_auc\": \"mean\", \n",
    "        \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", \n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_perspective.columns = ['_'.join(col).strip('_') for col in averages_perspective.columns]\n",
    "averages_perspective = averages_perspective.reset_index()\n",
    "\n",
    "averages_perspective[\"perspective\"] = \"All\"\n",
    "\n",
    "\n",
    "result_df = pd.concat([result_methods_df, averages_perspective], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_methods_df.shape)\n",
    "print(averages_perspective.shape)\n",
    "print(result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional columns used in analysis\n",
    "result_df[\"dataset_size\"] = result_df[\"dataset\"].str.split('_').str[3]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"dataset\"].str.split('_').str[4]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"anomaly_percentage\"].astype(float)\n",
    "result_df[\"batch_size\"] = result_df[\"batch_size\"].astype(str)\n",
    "result_df[\"vector_size\"] = result_df[\"vector_size\"].astype(str)\n",
    "result_df[\"vector_window_size\"] = result_df[\"vector_size\"].astype(str) + '/' + result_df[\"window_size\"].astype(str)\n",
    "result_df[\"prefix\"] = result_df[\"prefix\"].astype(str)\n",
    "result_df[\"prefix_buckets\"] = result_df[\"prefix\"].astype(str) + '/' + result_df[\"buckets\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_single_score(data, score_type, level, ax, label_name, xlabel_name):\n",
    "    subset = data[data[\"level\"] == level]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=label_name,\n",
    "        y=score_type,\n",
    "        hue=\"perspective\",\n",
    "        style=\"categorical_encoding\",\n",
    "        markers=True,\n",
    "        dashes=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{score_type.capitalize()} Scores (Level={level.capitalize()})\")\n",
    "    ax.set_xlabel(xlabel_name)\n",
    "    ax.set_ylabel(f\"{score_type.capitalize()}\" if level=='trace' else \"\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "def plot_scores(data, directory, title, label_name, xlabel_name, summary=True, filter_beginning_percentage=0, postfix=None):\n",
    "    levels = [\"trace\", \"event\", \"attribute\"]\n",
    "    \n",
    "    if summary:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 8), sharex=True, sharey=False)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(6, 3, figsize=(18, 18), sharex=True, sharey=False)\n",
    "    \n",
    "    handles, labels = [], []\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        plot_single_score(data, \"f1\", level, axes[0, i], label_name, xlabel_name)\n",
    "        plot_single_score(data, \"run_time\", level, axes[1, i], label_name, xlabel_name)\n",
    "        if not summary:\n",
    "            plot_single_score(data, \"pr_auc\", level, axes[2, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"roc_auc\", level, axes[3, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"precision\", level, axes[4, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"recall\", level, axes[5, i], label_name, xlabel_name)\n",
    "\n",
    "        if not handles and not labels:\n",
    "            handles, labels = axes[0, i].get_legend_handles_labels()\n",
    "        \n",
    "        axes[0, i].legend().remove()\n",
    "        axes[1, i].legend().remove()\n",
    "        if not summary:\n",
    "            axes[2, i].legend().remove()\n",
    "            axes[3, i].legend().remove()\n",
    "            axes[4, i].legend().remove()\n",
    "            axes[5, i].legend().remove()\n",
    "\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Perspective & Encoding\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        ncol=3\n",
    "    )\n",
    "    \n",
    "    fig.suptitle(f'F1-Scores: {directory}: {title}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "\n",
    "    plot_path = f\"plots\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    if postfix is not None:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{postfix}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Experiment_Anomaly_Percentage\" in directory:\n",
    "    xlabel_name=\"Anomaly Percentages\"\n",
    "    label_name=\"anomaly_percentage\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Synthetic_Dataset\" in directory:\n",
    "    xlabel_name=\"Dataset Sizes\"\n",
    "    label_name=\"dataset_size\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Batch_Size\" in directory:\n",
    "    xlabel_name=\"Batch Sizes\"\n",
    "    label_name=\"batch_size\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Prefix_v2\" in directory:\n",
    "    xlabel_name=\"Prefix/Buckets\"\n",
    "    label_name=\"prefix_buckets\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "   \n",
    "elif \"Experiment_Finetuning_Fixed_Vector_Vector_Sizes\" in directory:\n",
    "    filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Fixed Vector\"]\n",
    "\n",
    "    xlabel_name=\"Vector Sizes\"\n",
    "    label_name=\"vector_size\"\n",
    "    plot_scores(filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  \n",
    "\n",
    "elif \"Experiment_Finetuning_T2V_Window_Vector_Sizes\" in directory:\n",
    "    atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Average Then Concatinate\"]\n",
    "    c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Concatinate\"]\n",
    "\n",
    "    xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "    label_name=\"vector_window_size\"\n",
    "    plot_scores(atc_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    xlabel_name=\"Vector/Window Sizes C\"\n",
    "    plot_scores(c_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "    plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')      \n",
    "      \n",
    "elif \"Experiment_Finetuning_W2V_Window_Vector_Sizes\" in directory:\n",
    "    atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Average Then Concatinate\"]\n",
    "    c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Concatinate\"]\n",
    "\n",
    "    xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "    label_name=\"vector_window_size\"\n",
    "    plot_scores(atc_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    xlabel_name=\"Vector/Window Sizes C\"\n",
    "    plot_scores(c_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "    plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df[\"dataset_short_name\"] = all_scores_df[\"dataset\"].str.split('_').str[3] + \"/\" + all_scores_df[\"dataset\"].str.split('_').str[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_levels = (\n",
    "    all_scores_df.groupby(['dataset_short_name', 'categorical_encoding', 'level'])[['f1']]\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_combined_levels = (\n",
    "    all_scores_df.groupby(['dataset_short_name', 'categorical_encoding'])[['f1']]\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_combined_levels['level'] = 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_combined_levels['index'] = [None] \n",
    "grouped_levels = pd.concat([grouped_all_levels, grouped_combined_levels], axis=0, ignore_index=True)\n",
    "grouped_levels.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in grouped_levels.columns]\n",
    "grouped_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels['f1_rank'] = grouped_levels.groupby(['dataset_short_name', 'level'])['f1_mean'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "grouped_levels.to_csv(f'{plot_path}\\grouped_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "g = sns.FacetGrid(grouped_levels, col=\"level\", sharey=True, sharex=False, height=5, aspect=1.5, col_wrap=2)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"dataset_short_name\",\n",
    "    y=\"f1_rank\",\n",
    "    hue=\"categorical_encoding\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Dataset\", \"Rank (1 = Best)\")\n",
    "g.set_titles(\"Level: {col_name}\")\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.figure.suptitle(f\"{directory}: Ranking of Categorical Encodings methods per Dataset and Level\", fontsize=16)\n",
    "\n",
    "# for ax in g.axes.flatten():\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "#     ax.grid(False)\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    # Get the current tick positions and labels\n",
    "    tick_positions = ax.get_xticks()\n",
    "    tick_labels = ax.get_xticklabels()\n",
    "\n",
    "    # Set the ticks and labels explicitly with rotation\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.grid(False)  # Turn off gridlines\n",
    "    \n",
    "g.add_legend(title=\"Categorical Encoding\", loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "plt.savefig(f\"{plot_path}\\encoding_ranking_per_dataset.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_stats_df = (\n",
    "    grouped_levels.groupby(['categorical_encoding', 'level'])\n",
    "    .agg(\n",
    "        rank_mean=('f1_rank', 'mean'),\n",
    "        rank_std=('f1_rank', 'std'),\n",
    "        f1_mean=('f1_mean', 'mean'),\n",
    "        f1_std=('f1_mean', 'std')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rank_stats_df['rank_std'] = rank_stats_df['rank_std'].fillna(0)\n",
    "rank_stats_df['f1_std'] = rank_stats_df['f1_std'].fillna(0)\n",
    "\n",
    "rank_stats_df['rank_mean'] = rank_stats_df['rank_mean'].round(2)\n",
    "rank_stats_df['rank_std'] = rank_stats_df['rank_std'].round(2)\n",
    "rank_stats_df['f1_mean'] = rank_stats_df['f1_mean'].round(2)\n",
    "rank_stats_df['f1_std'] = rank_stats_df['f1_std'].round(2)\n",
    "\n",
    "rank_stats_df = rank_stats_df.sort_values(by=['level', 'rank_mean'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "rank_stats_df.to_csv(f'{plot_path}\\summarised_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rankings(data_df, title, rank_column='rank_mean', sharey=True):\n",
    "    levels = ['combined', 'trace', 'event', 'attribute']\n",
    "    fig, axes = plt.subplots(1, len(levels), figsize=(15, 4), sharey=sharey)\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        ax = axes[i]\n",
    "        level_data = data_df[data_df['level'] == level]\n",
    "        \n",
    "        ordered_categories = level_data.sort_values(rank_column)['categorical_encoding']\n",
    "        \n",
    "        ax.bar(ordered_categories, level_data.sort_values(rank_column)[rank_column], color='skyblue')\n",
    "\n",
    "        ax.set_title(f\"Level: {level}\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Average Rank (Lower = Better)\")\n",
    "        ax.tick_params(axis=\"x\", rotation=90)\n",
    "        ax.grid(False)\n",
    "        ax.grid(axis='y', linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "\n",
    "    # plt.xticks(rotation=45)\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    fig.suptitle(f\"{directory}: Average {title} Categorical Encoding methods per Level\", fontsize=16)\n",
    "\n",
    "    plot_path = f\"plots\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    plt.savefig(f\"{plot_path}\\encoding_rankings_summary_{rank_column}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rankings(rank_stats_df, title='Rank', rank_column='rank_mean', sharey=True)\n",
    "plot_rankings(rank_stats_df, title='F1', rank_column='f1_mean', sharey=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
