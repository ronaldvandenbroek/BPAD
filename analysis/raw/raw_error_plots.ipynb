{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\ronal\\\\OneDrive\\\\Documents\\\\GitHub\\\\BPAD', 'c:\\\\Users\\\\ronal\\\\OneDrive\\\\Documents\\\\GitHub\\\\BPAD\\\\analysis\\\\raw', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\python39.zip', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\DLLs', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad', '', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\setuptools\\\\_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..\\..')\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.fs import RESULTS_RAW_DIR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_subfolders(experiment_name):\n",
    "    experiment_path = os.path.join(RESULTS_RAW_DIR, experiment_name)\n",
    "    # Get all subfolder names in the specified directory\n",
    "    run_names = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name))]\n",
    "    return run_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(run_name, verbose=False,directory=None):\n",
    "    if directory:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "    else:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, run_name)\n",
    "    npy_files = [file for file in os.listdir(run_path) if file.endswith('.npy')]\n",
    "\n",
    "    loaded_data = {}\n",
    "\n",
    "    # Load each .npy file and use the file name (without extension) as the key\n",
    "    for npy_file in npy_files:\n",
    "        file_path = os.path.join(run_path, npy_file)\n",
    "        key = os.path.splitext(npy_file)[0]  # Get the file name without .npy extension\n",
    "        loaded_data[key] = np.load(file_path)\n",
    "\n",
    "        if verbose: print(f\"{loaded_data[key].shape} \\t {key}\")\n",
    "\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets(keys):\n",
    "    buckets = set()\n",
    "    for key in keys:\n",
    "        numbers = re.findall(r'\\d+', key)\n",
    "        buckets.update(map(int, numbers))\n",
    "    if len(buckets) > 0:\n",
    "        return sorted(buckets)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results, labels, directory, run_name, perspective, level, bucket=None, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    def scatter_plot(ax, results, labels):\n",
    "        y_values = results\n",
    "        x_values = np.arange(len(results))\n",
    "        ax.scatter(x_values[labels == 0], y_values[labels == 0], c='grey', s=3, label='Normal Prefixes', zorder=1)\n",
    "        ax.scatter(x_values[labels == 1], y_values[labels == 1], c='red', s=3, label='Anomalous Prefixes', zorder=2)\n",
    "        ax.grid(True)\n",
    "\n",
    "    subtitle = f'{directory}     {run_name}'\n",
    "    if len(results) == 0:\n",
    "        print(f'ERROR no results found for {subtitle}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        labels = labels[:, perspective]\n",
    "        scatter_plot(ax, results, labels)\n",
    "        \n",
    "        perspective_name = Perspective.values()[perspective]\n",
    "\n",
    "        bucket_string = ''\n",
    "        if bucket is not None:\n",
    "            bucket_string = f'with bucket size {str(bucket)}'\n",
    "        \n",
    "        title = f'Error per Prefix on the {perspective_name} perspective at {level} level {bucket_string}'\n",
    "        \n",
    "        # Print to keep track of plotting\n",
    "        print(f'\\t {title}')\n",
    "        \n",
    "        plt.title(f'{title}\\n{subtitle}')\n",
    "        plt.xlabel('Prefix Index')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        if zoom:\n",
    "            axins = inset_axes(ax, width=\"60%\", height=\"60%\", loc='upper right')\n",
    "\n",
    "            scatter_plot(axins, results, labels)\n",
    "            axins.set_xlim(zoom[0])\n",
    "            axins.set_ylim(zoom[1])\n",
    "            _,_ = ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=3)\n",
    "\n",
    "        plt.xlabel('Case Index')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        plot_path = f\"plots\\{directory}\\{run_name} \"\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        plt.savefig(f\"{plot_path}\\{perspective_name}_{level}_{bucket_string}.png\", format='png', dpi=300)\n",
    "        \n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def bucket_plot_losses(results_name, labels_name, run_name, directory, bucket_lengths, results, perspective, level, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    if bucket_lengths is None:\n",
    "        plot_losses(\n",
    "            results=results[f'{results_name}'], \n",
    "            labels=results[f'{labels_name}'],\n",
    "            directory=directory,\n",
    "            run_name=run_name, perspective=perspective, level=level, bucket=None, zoom=zoom, show_plots=show_plots)       \n",
    "    else:\n",
    "        for bucket in bucket_lengths:\n",
    "            plot_losses(\n",
    "                results=results[f'{results_name}_{bucket}'], \n",
    "                labels=results[f'{labels_name}_{bucket}'],\n",
    "                directory=directory,\n",
    "                run_name=run_name, perspective=perspective, level=level, bucket=bucket, zoom=zoom, show_plots=show_plots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Some plots seem to be empty, check if this is due to a data saving or plotting error\n",
    "# TODO: Rerun DAE_Finetuned_Embedding with latest modifications to easily show config in plot\n",
    "# TODO: Run on the event level\n",
    "\n",
    "# TODO: BPIC2015 workload seems to be bugged on the datagen level (also present with the synthetic dataset)\n",
    "# Seems to be a problem with division errors during AD\n",
    "\n",
    "# TODO: Seems to be an error when using no buckets with bpic2015 and ATC\n",
    "# ValueError: operands could not be broadcast together with shapes (35483,1065) (35483,8060)\n",
    "\n",
    "# TODO: Run bpic without bucketing\n",
    "# TODO: Run DEA in a more restrictive hidden layer config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAE_finetuned_embedding_batch_size_1\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    \"Trace2Vec_Synthetic\", # Contains synthetic Trace2Vec encoding tests\n",
    "    'DAE_bpic2015_prefixes', # Contains real world no prefixes all encoding methods (no trace2vec)\n",
    "    'DAE_bpic2015_no_prefixes_v2', # Contains real world with prefixes all encoding methods (no trace2vec)\n",
    "    'DAE_Finetuned_Embedding', # Contains synthetic with prefixes all encoding methods (no trace2vec)\n",
    "    'DAE_bpic2015_no_buckets',\n",
    "    'DAE_bpic2015_no_buckets_real_world',\n",
    "    'DAE_finetuned_embedding_batch_size_1'] \n",
    "directory = directories[-1]\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24-11-06-13-13_DAE_EncodingCategorical.WORD_2_VEC_ATC_EncodingNumerical.MIN_MAX_SCALING']\n"
     ]
    }
   ],
   "source": [
    "run_list = list_subfolders(directory)\n",
    "print(run_list)\n",
    "\n",
    "# run_list = [run_list[0]]\n",
    "# print(run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_names = []\n",
    "run_results = []\n",
    "run_buckets = []\n",
    "for run_name in run_list:\n",
    "    results = load_results(run_name=run_name, directory=directory)\n",
    "    buckets = get_buckets(results.keys())\n",
    "\n",
    "    run_names.append(run_name)\n",
    "    run_results.append(results)\n",
    "    run_buckets.append(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_config = [\n",
    "    ('result_DAE_trace_Order', 'labels_DAE_trace', Perspective.ORDER, 'trace'),\n",
    "    ('result_DAE_trace_Attribute', 'labels_DAE_trace', Perspective.ATTRIBUTE, 'trace'),\n",
    "    ('result_DAE_trace_Arrival Time', 'labels_DAE_trace', Perspective.ARRIVAL_TIME, 'trace'),\n",
    "    ('result_DAE_trace_Workload', 'labels_DAE_trace', Perspective.WORKLOAD, 'trace'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: DAE_finetuned_embedding_batch_size_1\t24-11-06-13-13_DAE_EncodingCategorical.WORD_2_VEC_ATC_EncodingNumerical.MIN_MAX_SCALING\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 3\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 4\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 5\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 6\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 7\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 8\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 9\n",
      "\t Error per Prefix on the Order perspective at trace level with bucket size 13\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 3\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 4\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 5\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 6\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 7\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 8\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 9\n",
      "\t Error per Prefix on the Attribute perspective at trace level with bucket size 13\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 3\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 4\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 5\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 6\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 7\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 8\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 9\n",
      "\t Error per Prefix on the Arrival Time perspective at trace level with bucket size 13\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 3\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 4\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 5\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 6\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 7\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 8\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 9\n",
      "\t Error per Prefix on the Workload perspective at trace level with bucket size 13\n"
     ]
    }
   ],
   "source": [
    "show_plots = False\n",
    "for run_name, run_result, run_bucket in zip(run_names, run_results, run_buckets):\n",
    "    print(f\"Generating: {directory}\\t{run_name}\")\n",
    "    for config in results_config:\n",
    "        # try:\n",
    "            bucket_plot_losses(\n",
    "                results_name=config[0], \n",
    "                labels_name=config[1],\n",
    "                directory=directory,\n",
    "                run_name=run_name,\n",
    "                bucket_lengths=run_bucket,\n",
    "                results=run_result,\n",
    "                perspective=config[2],\n",
    "                level=config[3],\n",
    "                zoom=None,\n",
    "                show_plots=show_plots)\n",
    "        # except:\n",
    "        #     print(\"Error loading \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
