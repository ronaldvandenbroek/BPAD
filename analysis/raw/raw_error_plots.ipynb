{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\ronal\\\\OneDrive\\\\Documents\\\\GitHub\\\\BPAD', 'c:\\\\Users\\\\ronal\\\\OneDrive\\\\Documents\\\\GitHub\\\\BPAD\\\\analysis\\\\raw', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\python39.zip', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\DLLs', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad', '', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\ronal\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ronal\\\\miniconda3\\\\envs\\\\rcvdb-thesis-bpad\\\\lib\\\\site-packages\\\\setuptools\\\\_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..\\..')\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.fs import RESULTS_RAW_DIR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_subfolders(experiment_name):\n",
    "    experiment_path = os.path.join(RESULTS_RAW_DIR, experiment_name)\n",
    "    # Get all subfolder names in the specified directory\n",
    "    run_names = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name))]\n",
    "    return run_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_results(run_name, verbose=False,directory=None):\n",
    "    if directory:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "    else:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, run_name)\n",
    "    npy_files = [file for file in os.listdir(run_path) if file.endswith('.npy')]\n",
    "\n",
    "    loaded_data = {}\n",
    "\n",
    "    # Load each .npy file and use the file name (without extension) as the key\n",
    "    for npy_file in npy_files:\n",
    "        file_path = os.path.join(run_path, npy_file)\n",
    "        key = os.path.splitext(npy_file)[0]  # Get the file name without .npy extension\n",
    "        loaded_data[key] = np.load(file_path)\n",
    "\n",
    "        if verbose: print(f\"{loaded_data[key].shape} \\t {key}\")\n",
    "\n",
    "    return loaded_data\n",
    "\n",
    "def load_config(run_name, directory=None):\n",
    "    if directory:\n",
    "        config_path = os.path.join(RESULTS_RAW_DIR, directory, run_name, \"config.json\")\n",
    "    else:\n",
    "        config_path = os.path.join(RESULTS_RAW_DIR, run_name, \"config.json\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def get_buckets(keys):\n",
    "    buckets = set()\n",
    "    for key in keys:\n",
    "        numbers = re.findall(r'\\d+', key)\n",
    "        buckets.update(map(int, numbers))\n",
    "    if len(buckets) > 0:\n",
    "        return sorted(buckets)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results, labels, directory, run_name, perspective, level, bucket=None, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    def scatter_plot(ax, results, labels):\n",
    "        y_values = results\n",
    "        x_values = np.arange(len(results))\n",
    "        ax.scatter(x_values[labels == 0], y_values[labels == 0], c='grey', s=3, label='Normal Prefixes', zorder=1)\n",
    "        ax.scatter(x_values[labels == 1], y_values[labels == 1], c='red', s=3, label='Anomalous Prefixes', zorder=2)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Normalize results\n",
    "    results = np.interp(results, (results.min(), results.max()), (0, 1))\n",
    "\n",
    "    subtitle = f'{directory}     {run_name}'\n",
    "    if len(results) == 0:\n",
    "        print(f'ERROR no results found for {subtitle}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        labels = labels[:, perspective]\n",
    "        scatter_plot(ax, results, labels)\n",
    "        \n",
    "        perspective_name = Perspective.values()[perspective]\n",
    "\n",
    "        bucket_string = ''\n",
    "        if bucket is not None:\n",
    "            bucket_string = f'with bucket size {str(bucket)}'\n",
    "        \n",
    "        title = f'Error per Prefix on the {perspective_name} perspective at {level} level {bucket_string}'\n",
    "        \n",
    "        # Print to keep track of plotting\n",
    "        # print(f'\\t {title}')\n",
    "        \n",
    "        plt.title(f'{title}\\n{subtitle}')\n",
    "        plt.xlabel('Prefix Index')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        if zoom:\n",
    "            axins = inset_axes(ax, width=\"60%\", height=\"60%\", loc='upper right')\n",
    "\n",
    "            scatter_plot(axins, results, labels)\n",
    "            axins.set_xlim(zoom[0])\n",
    "            axins.set_ylim(zoom[1])\n",
    "            _,_ = ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=3)\n",
    "\n",
    "        plt.xlabel('Case Index')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        plot_path = f\"plots\\{directory}\\{run_name} \"\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        plt.savefig(f\"{plot_path}\\{perspective_name}_{level}_{bucket_string}.png\", format='png', dpi=300)\n",
    "        \n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def bucket_plot_losses(results_name, labels_name, run_name, directory, bucket_lengths, results, perspective, level, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True, pbar=None):\n",
    "    if bucket_lengths is None:\n",
    "        plot_losses(\n",
    "            results=results[f'{results_name}'], \n",
    "            labels=results[f'{labels_name}'],\n",
    "            directory=directory,\n",
    "            run_name=run_name, perspective=perspective, level=level, bucket=None, zoom=zoom, show_plots=show_plots)\n",
    "        if pbar:\n",
    "            pbar.update(1)       \n",
    "    else:\n",
    "        for bucket in bucket_lengths:\n",
    "            plot_losses(\n",
    "                results=results[f'{results_name}_{bucket}'], \n",
    "                labels=results[f'{labels_name}_{bucket}'],\n",
    "                directory=directory,\n",
    "                run_name=run_name, perspective=perspective, level=level, bucket=bucket, zoom=zoom, show_plots=show_plots)\n",
    "            if pbar:\n",
    "                pbar.update(1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Some plots seem to be empty, check if this is due to a data saving or plotting error\n",
    "# TODO: Rerun DAE_Finetuned_Embedding with latest modifications to easily show config in plot\n",
    "# TODO: Run on the event level\n",
    "\n",
    "# TODO: BPIC2015 workload seems to be bugged on the datagen level (also present with the synthetic dataset)\n",
    "# Seems to be a problem with division errors during AD\n",
    "\n",
    "# TODO: Seems to be an error when using no buckets with bpic2015 and ATC\n",
    "# ValueError: operands could not be broadcast together with shapes (35483,1065) (35483,8060)\n",
    "\n",
    "# TODO: Run bpic without bucketing\n",
    "# TODO: Run DEA in a more restrictive hidden layer config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAE_debug\n"
     ]
    }
   ],
   "source": [
    "# directories = [\n",
    "#     \"Trace2Vec_Synthetic\", # Contains synthetic Trace2Vec encoding tests\n",
    "#     'DAE_bpic2015_prefixes', # Contains real world no prefixes all encoding methods (no trace2vec)\n",
    "#     'DAE_bpic2015_no_prefixes_v2', # Contains real world with prefixes all encoding methods (no trace2vec)\n",
    "#     'DAE_Finetuned_Embedding', # Contains synthetic with prefixes all encoding methods (no trace2vec)\n",
    "#     'DAE_bpic2015_no_buckets',\n",
    "#     'DAE_bpic2015_no_buckets_real_world',\n",
    "#     'DAE_finetuned_embedding_batch_size_1'] \n",
    "directories = [\n",
    "    'DAE_debug'] \n",
    "directory = directories[-1]\n",
    "print(directory)\n",
    "\n",
    "score_results = True\n",
    "plot_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24-11-18-13-54_DAE_EncodingCategorical.WORD_2_VEC_ATC_EncodingNumerical.MIN_MAX_SCALING']\n"
     ]
    }
   ],
   "source": [
    "run_list = list_subfolders(directory)\n",
    "print(run_list)\n",
    "\n",
    "# run_list = [run_list[0]]\n",
    "# print(run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for run_name in run_list:\n",
    "    results = load_results(run_name=run_name, directory=directory)\n",
    "    config = load_config(run_name=run_name, directory=directory)\n",
    "    buckets = get_buckets(results.keys())\n",
    "\n",
    "    runs.append({\n",
    "        \"name\": run_name,\n",
    "        \"results\": results,\n",
    "        \"config\": config,\n",
    "        \"buckets\": buckets\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_config = [\n",
    "    ('result_DAE_trace_Order', 'labels_DAE_trace', Perspective.ORDER, 'trace'),\n",
    "    ('result_DAE_trace_Attribute', 'labels_DAE_trace', Perspective.ATTRIBUTE, 'trace'),\n",
    "    ('result_DAE_trace_Arrival Time', 'labels_DAE_trace', Perspective.ARRIVAL_TIME, 'trace'),\n",
    "    ('result_DAE_trace_Workload', 'labels_DAE_trace', Perspective.WORKLOAD, 'trace'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Plots: 100%|██████████| 32/32 [00:14<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "if plot_results:\n",
    "    show_plots = False\n",
    "\n",
    "    nr_buckets = 0\n",
    "    for run in runs:\n",
    "        nr_buckets += len(run[\"buckets\"])\n",
    "\n",
    "    total_iterations = nr_buckets * len(results_config)\n",
    "    with tqdm(total=total_iterations, desc=\"Generating Plots\") as pbar:\n",
    "        for run in runs:\n",
    "            # print(f\"Generating: {directory}\\t{run_name}\")\n",
    "            for config in results_config:\n",
    "                # try:\n",
    "                bucket_plot_losses(\n",
    "                    results_name=config[0], \n",
    "                    labels_name=config[1],\n",
    "                    directory=directory,\n",
    "                    run_name=run[\"name\"],\n",
    "                    bucket_lengths=run[\"buckets\"],\n",
    "                    results=run[\"results\"],\n",
    "                    perspective=config[2],\n",
    "                    level=config[3],\n",
    "                    zoom=None,\n",
    "                    show_plots=show_plots,\n",
    "                    pbar=pbar)\n",
    "                # except:\n",
    "                #     print(\"Error loading \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes_by_value(arr):\n",
    "    value_to_indexes = {}\n",
    "    for index, value in enumerate(arr):\n",
    "        if value not in value_to_indexes:\n",
    "            value_to_indexes[value] = []\n",
    "        value_to_indexes[value].append(index)\n",
    "    return value_to_indexes\n",
    "\n",
    "def normalize(array):\n",
    "    array = np.array(array)\n",
    "    return np.interp(array, (array.min(), array.max()), (0, 1))\n",
    "\n",
    "# Function to extract the number after the last underscore\n",
    "def extract_number(key):\n",
    "    return int(key.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attribute_labels(output, values, case_length, perspective, perspective_label_indices):\n",
    "    # print(values.shape)\n",
    "\n",
    "    perspective_value = values[perspective, :, :, :]\n",
    "    # print(perspective_value.shape)\n",
    "\n",
    "    perspective_masked = perspective_value[:, :case_length, :]\n",
    "    # print(perspective_masked.shape)\n",
    "\n",
    "    perspective_indexed = perspective_masked[:,:,perspective_label_indices[perspective]]\n",
    "    # print(perspective_indexed.shape)\n",
    "\n",
    "    perspective_attribute_value = perspective_indexed.reshape(-1) # Flatten the output\n",
    "    # print(perspective_attribute_value.shape)\n",
    "\n",
    "    output.append(perspective_attribute_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data_for_scoring(results, perspective_label_indices):\n",
    "    labels_DAE_attribute_Arrival_Time = []\n",
    "    labels_DAE_attribute_Workload = []\n",
    "    labels_DAE_attribute_Order = []\n",
    "    labels_DAE_attribute_Attribute  = []\n",
    "\n",
    "    labels_DAE_event = []\n",
    "    labels_DAE_trace = []\n",
    "\n",
    "    result_DAE_attribute_Arrival_Time = []\n",
    "    result_DAE_event_Arrival_Time = []\n",
    "    result_DAE_trace_Arrival_Time = []\n",
    "    result_DAE_attribute_Workload = []\n",
    "    result_DAE_event_Workload = []\n",
    "    result_DAE_trace_Workload = []\n",
    "    result_DAE_attribute_Order = []\n",
    "    result_DAE_event_Order = []\n",
    "    result_DAE_trace_Order = []\n",
    "    result_DAE_attribute_Attribute = []\n",
    "    result_DAE_event_Attribute = []\n",
    "    result_DAE_trace_Attribute = []\n",
    "\n",
    "    for (key, value) in results.items():\n",
    "        # print(key, value.shape)\n",
    "\n",
    "        length = int(key.split('_')[-1])\n",
    "        perspective = key.split('_')[-2]\n",
    "        if 'losses' in key:\n",
    "            continue\n",
    "        elif 'labels' in key:\n",
    "            if 'attribute' in key:\n",
    "                transposed_value = np.transpose(value, (3,0,1,2))# [:, :, :length, :]\n",
    "\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Arrival_Time,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ARRIVAL_TIME,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Attribute,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ATTRIBUTE,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Order,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.ORDER,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "                process_attribute_labels(\n",
    "                    output=labels_DAE_attribute_Workload,\n",
    "                    values=transposed_value, \n",
    "                    case_length=length, \n",
    "                    perspective=Perspective.WORKLOAD,\n",
    "                    perspective_label_indices=perspective_label_indices)\n",
    "\n",
    "                # # print(perspective_value.shape)\n",
    "                # perspective_value = perspective_value.reshape(perspective_value.shape[0], -1)\n",
    "                # # print(perspective_value.shape)\n",
    "                # labels_DAE_attribute.append(perspective_value)\n",
    "            elif 'event' in key:\n",
    "                perspective_value = np.transpose(value, (2,0,1))[:, :, :length]\n",
    "                perspective_value = perspective_value.reshape(perspective_value.shape[0], -1)\n",
    "                labels_DAE_event.append(perspective_value)\n",
    "            elif 'trace' in key:\n",
    "                perspective_value = np.transpose(value, (1,0))\n",
    "                labels_DAE_trace.append(perspective_value)\n",
    "        elif 'result' in key:\n",
    "            if 'attribute' in key:\n",
    "                # print(value.shape)\n",
    "                # value_max = np.max(value, axis=2)\n",
    "                # print(value.shape, normalize(value.reshape(-1)).shape, perspective)\n",
    "                # print(value.shape)\n",
    "                value = normalize(value.reshape(-1))\n",
    "                # print(value.shape)\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_attribute_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_attribute_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_attribute_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_attribute_Attribute.append(value)\n",
    "            if 'event' in key:\n",
    "                value = normalize(value.reshape(-1))\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_event_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_event_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_event_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_event_Attribute.append(value)\n",
    "            elif 'trace' in key:\n",
    "                value = normalize(value)\n",
    "                if 'Arrival Time' in perspective:\n",
    "                    result_DAE_trace_Arrival_Time.append(value)\n",
    "                elif 'Order' in perspective:\n",
    "                    result_DAE_trace_Order.append(value)\n",
    "                elif 'Workload' in perspective:\n",
    "                    result_DAE_trace_Workload.append(value)\n",
    "                elif 'Attribute' in perspective:\n",
    "                    result_DAE_trace_Attribute.append(value)\n",
    "\n",
    "\n",
    "    # labels_DAE_attribute = np.concatenate(labels_DAE_attribute, axis=1)\n",
    "    labels_DAE_event = np.concatenate(labels_DAE_event, axis=1)    \n",
    "    labels_DAE_trace = np.concatenate(labels_DAE_trace, axis=1)\n",
    "\n",
    "    # print(labels_DAE_attribute.shape)\n",
    "\n",
    "    # print(np.concatenate(result_DAE_event_Order, axis=0).shape)\n",
    "    # print(result_DAE_attribute_Attribute.shape)\n",
    "    # print(result_DAE_attribute_Arrival_Time.shape)\n",
    "    # print(result_DAE_attribute_Workload.shape)\n",
    "\n",
    "    labels_DAE_attribute = [\n",
    "        np.concatenate(labels_DAE_attribute_Order, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Attribute, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Arrival_Time, axis=0),\n",
    "        np.concatenate(labels_DAE_attribute_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_attribute = [\n",
    "        np.concatenate(result_DAE_attribute_Order, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_attribute_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_event = [\n",
    "        np.concatenate(result_DAE_event_Order, axis=0),\n",
    "        np.concatenate(result_DAE_event_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_event_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_event_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    result_DAE_trace = [\n",
    "        np.concatenate(result_DAE_trace_Order, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Attribute, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Arrival_Time, axis=0),\n",
    "        np.concatenate(result_DAE_trace_Workload, axis=0)\n",
    "    ]\n",
    "\n",
    "    return labels_DAE_attribute, labels_DAE_event, labels_DAE_trace, result_DAE_attribute, result_DAE_event, result_DAE_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def score_results(y_trues, pred_probs, perspective):\n",
    "    y_true = y_trues[perspective][:]\n",
    "    pred_prob = pred_probs[perspective][:]\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, pred_prob)\n",
    "\n",
    "\n",
    "    # PR-AUC\n",
    "    pr_auc = average_precision_score(y_true, pred_prob)\n",
    "\n",
    "    return roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def score(run):\n",
    "    results = run['results']\n",
    "    config = run['config']\n",
    "\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda x: extract_number(x[0])))\n",
    "    perspective_label_indices = get_indexes_by_value(config['attribute_perspectives_original'])\n",
    "\n",
    "    (\n",
    "        labels_DAE_attribute, \n",
    "        labels_DAE_event, \n",
    "        labels_DAE_trace, \n",
    "        result_DAE_attribute, \n",
    "        result_DAE_event, \n",
    "        result_DAE_trace\n",
    "    ) = reshape_data_for_scoring(results=sorted_results, perspective_label_indices=perspective_label_indices)\n",
    "\n",
    "    level = ['trace', 'event', 'attribute']\n",
    "    datasets = [labels_DAE_trace, labels_DAE_event, labels_DAE_attribute]\n",
    "    results = [result_DAE_trace, result_DAE_event, result_DAE_attribute]\n",
    "    perspectives = Perspective.keys()\n",
    "\n",
    "    scores = []\n",
    "    for (level, dataset, result), perspective in itertools.product(zip(level, datasets, results), perspectives):\n",
    "        try:\n",
    "            roc_auc, pr_auc = score_results(dataset, result, perspective)\n",
    "            # print(level, perspective, roc_auc, pr_auc)\n",
    "\n",
    "            scores.append({\n",
    "                # High level differentiatiors\n",
    "                'run_name':config['run_name'],\n",
    "                'model':config['model'],\n",
    "                'dataset':config['dataset'],\n",
    "                'repeat':config['repeat'],\n",
    "                # Level/Perspectives\n",
    "                'level': level,\n",
    "                'perspective': Perspective.values()[perspective],\n",
    "                # Scores\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'run_time': config['run_time'],\n",
    "                # Config\n",
    "                'batch_size':config['batch_size'],\n",
    "                'prefix':config['prefix'],\n",
    "                'buckets':config['bucket_boundaries'],\n",
    "                'categorical_encoding':config['categorical_encoding'],\n",
    "                'numerical_encoding':config['numerical_encoding'],\n",
    "                'vector_size':config['vector_size'],\n",
    "                'window_size':config['window_size'] \n",
    "            })\n",
    "        except:\n",
    "            print(level, perspective)\n",
    "\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in runs:\n",
    "if score_results:\n",
    "    scores_df = score(run=runs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>repeat</th>\n",
       "      <th>level</th>\n",
       "      <th>perspective</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>run_time</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>prefix</th>\n",
       "      <th>buckets</th>\n",
       "      <th>categorical_encoding</th>\n",
       "      <th>numerical_encoding</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>window_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>trace</td>\n",
       "      <td>Order</td>\n",
       "      <td>0.613941</td>\n",
       "      <td>0.056915</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>trace</td>\n",
       "      <td>Attribute</td>\n",
       "      <td>0.537034</td>\n",
       "      <td>0.051473</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>trace</td>\n",
       "      <td>Arrival Time</td>\n",
       "      <td>0.516534</td>\n",
       "      <td>0.045274</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>trace</td>\n",
       "      <td>Workload</td>\n",
       "      <td>0.815386</td>\n",
       "      <td>0.076581</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>event</td>\n",
       "      <td>Order</td>\n",
       "      <td>0.564894</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>event</td>\n",
       "      <td>Attribute</td>\n",
       "      <td>0.528422</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>event</td>\n",
       "      <td>Arrival Time</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>event</td>\n",
       "      <td>Workload</td>\n",
       "      <td>0.814324</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Order</td>\n",
       "      <td>0.564894</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Attribute</td>\n",
       "      <td>0.526387</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Arrival Time</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DAE_debug</td>\n",
       "      <td>DAE</td>\n",
       "      <td>medium_debug_v2-0.15-4_1.json.gz</td>\n",
       "      <td>None</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Workload</td>\n",
       "      <td>0.825552</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>71.211199</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 13]</td>\n",
       "      <td>Word2Vec Average Then Concatinate</td>\n",
       "      <td>Min Max Scaling</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     run_name model                           dataset repeat      level  \\\n",
       "0   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      trace   \n",
       "1   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      trace   \n",
       "2   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      trace   \n",
       "3   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      trace   \n",
       "4   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      event   \n",
       "5   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      event   \n",
       "6   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      event   \n",
       "7   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None      event   \n",
       "8   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None  attribute   \n",
       "9   DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None  attribute   \n",
       "10  DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None  attribute   \n",
       "11  DAE_debug   DAE  medium_debug_v2-0.15-4_1.json.gz   None  attribute   \n",
       "\n",
       "     perspective   roc_auc    pr_auc   run_time  batch_size  prefix  \\\n",
       "0          Order  0.613941  0.056915  71.211199           8    True   \n",
       "1      Attribute  0.537034  0.051473  71.211199           8    True   \n",
       "2   Arrival Time  0.516534  0.045274  71.211199           8    True   \n",
       "3       Workload  0.815386  0.076581  71.211199           8    True   \n",
       "4          Order  0.564894  0.012094  71.211199           8    True   \n",
       "5      Attribute  0.528422  0.014549  71.211199           8    True   \n",
       "6   Arrival Time  0.506849  0.009656  71.211199           8    True   \n",
       "7       Workload  0.814324  0.011861  71.211199           8    True   \n",
       "8          Order  0.564894  0.012094  71.211199           8    True   \n",
       "9      Attribute  0.526387  0.005618  71.211199           8    True   \n",
       "10  Arrival Time  0.506849  0.009656  71.211199           8    True   \n",
       "11      Workload  0.825552  0.002850  71.211199           8    True   \n",
       "\n",
       "                      buckets               categorical_encoding  \\\n",
       "0   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "1   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "2   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "3   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "4   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "5   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "6   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "7   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "8   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "9   [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "10  [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "11  [3, 4, 5, 6, 7, 8, 9, 13]  Word2Vec Average Then Concatinate   \n",
       "\n",
       "   numerical_encoding  vector_size  window_size  \n",
       "0     Min Max Scaling          200           10  \n",
       "1     Min Max Scaling          200           10  \n",
       "2     Min Max Scaling          200           10  \n",
       "3     Min Max Scaling          200           10  \n",
       "4     Min Max Scaling          200           10  \n",
       "5     Min Max Scaling          200           10  \n",
       "6     Min Max Scaling          200           10  \n",
       "7     Min Max Scaling          200           10  \n",
       "8     Min Max Scaling          200           10  \n",
       "9     Min Max Scaling          200           10  \n",
       "10    Min Max Scaling          200           10  \n",
       "11    Min Max Scaling          200           10  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
