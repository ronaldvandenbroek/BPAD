{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..\\..')\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results, labels, directory, run_name, perspective, level, bucket=None, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    def scatter_plot(ax, results, labels):\n",
    "        y_values = results\n",
    "        x_values = np.arange(len(results))\n",
    "        ax.scatter(x_values[labels == 0], y_values[labels == 0], c='grey', s=3, label='Normal Prefixes', zorder=1)\n",
    "        ax.scatter(x_values[labels == 1], y_values[labels == 1], c='red', s=3, label='Anomalous Prefixes', zorder=2)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Normalize results\n",
    "    results = np.interp(results, (results.min(), results.max()), (0, 1))\n",
    "\n",
    "    subtitle = f'{directory}     {run_name}'\n",
    "    if len(results) == 0:\n",
    "        print(f'ERROR no results found for {subtitle}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        labels = labels[:, perspective]\n",
    "        scatter_plot(ax, results, labels)\n",
    "        \n",
    "        perspective_name = Perspective.values()[perspective]\n",
    "\n",
    "        bucket_string = ''\n",
    "        if bucket is not None:\n",
    "            bucket_string = f'with bucket size {str(bucket)}'\n",
    "        \n",
    "        title = f'Error per Prefix on the {perspective_name} perspective at {level} level {bucket_string}'\n",
    "        \n",
    "        # Print to keep track of plotting\n",
    "        # print(f'\\t {title}')\n",
    "        \n",
    "        plt.title(f'{title}\\n{subtitle}')\n",
    "        plt.xlabel('Prefix Index')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        if zoom:\n",
    "            axins = inset_axes(ax, width=\"60%\", height=\"60%\", loc='upper right')\n",
    "\n",
    "            scatter_plot(axins, results, labels)\n",
    "            axins.set_xlim(zoom[0])\n",
    "            axins.set_ylim(zoom[1])\n",
    "            _,_ = ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=3)\n",
    "\n",
    "        plt.xlabel('Case Index')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        plot_path = f\"plots\\{directory}\\{run_name} \"\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        plt.savefig(f\"{plot_path}\\error_plots\\{perspective_name}_{level}_{bucket_string}.png\", format='png', dpi=300)\n",
    "        \n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def bucket_plot_losses(results_name, labels_name, run_name, directory, bucket_lengths, results, perspective, level, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True, pbar=None):\n",
    "    if bucket_lengths is None:\n",
    "        plot_losses(\n",
    "            results=results[f'{results_name}'], \n",
    "            labels=results[f'{labels_name}'],\n",
    "            directory=directory,\n",
    "            run_name=run_name, perspective=perspective, level=level, bucket=None, zoom=zoom, show_plots=show_plots)\n",
    "        if pbar:\n",
    "            pbar.update(1)       \n",
    "    else:\n",
    "        for bucket in bucket_lengths:\n",
    "            plot_losses(\n",
    "                results=results[f'{results_name}_{bucket}'], \n",
    "                labels=results[f'{labels_name}_{bucket}'],\n",
    "                directory=directory,\n",
    "                run_name=run_name, perspective=perspective, level=level, bucket=bucket, zoom=zoom, show_plots=show_plots)\n",
    "            if pbar:\n",
    "                pbar.update(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_single_score(data, score_type, level, ax, label_name, xlabel_name):\n",
    "    subset = data[data[\"level\"] == level]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=label_name,\n",
    "        y=score_type,\n",
    "        hue=\"perspective\",\n",
    "        # style=\"categorical_encoding\",\n",
    "        markers=True,\n",
    "        dashes=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{score_type.capitalize()} Scores (Level={level.capitalize()})\")\n",
    "    ax.set_xlabel(xlabel_name)\n",
    "    ax.set_ylabel(f\"{score_type.capitalize()}\" if level=='trace' else \"\")\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "def plot_scores(data, directory, title, label_name, xlabel_name, summary=True, filter_beginning_percentage=0, postfix=None):\n",
    "    levels = [\"trace\", \"event\", \"attribute\"]\n",
    "    \n",
    "    if summary:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 8), sharex=True, sharey=False)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(6, 3, figsize=(18, 18), sharex=True, sharey=False)\n",
    "    \n",
    "    handles, labels = [], []\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        plot_single_score(data, \"f1\", level, axes[0, i], label_name, xlabel_name)\n",
    "        plot_single_score(data, \"run_time\", level, axes[1, i], label_name, xlabel_name)\n",
    "        if not summary:\n",
    "            plot_single_score(data, \"pr_auc\", level, axes[2, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"roc_auc\", level, axes[3, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"precision\", level, axes[4, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"recall\", level, axes[5, i], label_name, xlabel_name)\n",
    "\n",
    "        if not handles and not labels:\n",
    "            handles, labels = axes[0, i].get_legend_handles_labels()\n",
    "        \n",
    "        axes[0, i].legend().remove()\n",
    "        axes[1, i].legend().remove()\n",
    "        if not summary:\n",
    "            axes[2, i].legend().remove()\n",
    "            axes[3, i].legend().remove()\n",
    "            axes[4, i].legend().remove()\n",
    "            axes[5, i].legend().remove()\n",
    "\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Perspective & Encoding\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        ncol=3\n",
    "    )\n",
    "    \n",
    "    fig.suptitle(f'F1-Scores: {directory}: {title}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "\n",
    "    plot_path = f\"plots\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    if postfix is not None:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{postfix}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Experiment_Real_World_Debug',\n",
    "\n",
    "directories = [\n",
    "    # 'Experiment_Prefix_v2',\n",
    "    # 'Experiment_Batch_Size',\n",
    "    'Experiment_Anomaly_Percentage_v2',\n",
    "    # 'Experiment_Synthetic_Dataset_v4',\n",
    "\n",
    "    # 'Experiment_Finetuning_Fixed_Vector_Vector_Sizes',\n",
    "    # 'Experiment_Finetuning_T2V_Window_Vector_Sizes',\n",
    "    # 'Experiment_Finetuning_W2V_Window_Vector_Sizes',\n",
    "\n",
    "    # 'Experiment_Synthetic_All_Models',\n",
    "    # 'Experiment_Real_World_All_Models',\n",
    "\n",
    "    ] \n",
    "\n",
    "filter_beginning_percentage = 0\n",
    "\n",
    "recalculate = False\n",
    "\n",
    "score_results = True\n",
    "score_summary = True\n",
    "\n",
    "rank_encoders = True\n",
    "\n",
    "plot_results = False\n",
    "show_plots = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = directories[-1]\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.raw.utils.load_data import list_subfolders_or_zip_files\n",
    "\n",
    "\n",
    "run_list = list_subfolders_or_zip_files(directory)\n",
    "print(run_list)\n",
    "print(run_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.raw.utils.load_data import load_score_dataframe\n",
    "\n",
    "score_path = f\"plots\\{directory}\\\\\"\n",
    "score_file = f\"scores_raw_df.pkl\"\n",
    "all_scores_df = None\n",
    "\n",
    "# Check if scores_raw_df.pkl exists and if yes skip reloading all data, force recalculation if set to true\n",
    "if not recalculate:\n",
    "    all_scores_df = load_score_dataframe(score_path + score_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.raw.utils.load_data import cleanup_temp_folders, get_buckets, load_config, load_results, unzip_results\n",
    "from analysis.raw.utils.process_raw_data import score\n",
    "\n",
    "\n",
    "runs = []\n",
    "scores_dfs = []\n",
    "total_runtime = 0\n",
    "\n",
    "# If no scores_raw_df.pkl exists, recalulate is true or plotting individual runs start loading all data from raw\n",
    "if all_scores_df is None or recalculate is True or plot_results is True:\n",
    "    for index, run_name in enumerate(tqdm(run_list)):\n",
    "        try:\n",
    "            # If needed unzip the data\n",
    "            run_name, from_zip = unzip_results(directory, run_name)\n",
    "\n",
    "            # Loading the data\n",
    "            results = load_results(run_name=run_name, directory=directory)\n",
    "            config = load_config(run_name=run_name, directory=directory)\n",
    "            buckets = get_buckets(results.keys())\n",
    "            timestamp = run_name.split('_')[0]\n",
    "\n",
    "            total_runtime += config['run_time']\n",
    "\n",
    "            # If needed clean up temp folder\n",
    "            if from_zip:\n",
    "                cleanup_temp_folders(directory, run_name)\n",
    "            \n",
    "            # If set filter the first % of results from the run to allow the scoring some grace period\n",
    "            if filter_beginning_percentage != 0:\n",
    "                for key, value in results.items():\n",
    "                    filter_index = int(value.shape[0] / filter_beginning_percentage)\n",
    "                    # print(filter_index)\n",
    "                    results[key] = value[filter_index:]\n",
    "\n",
    "            run = {\n",
    "                \"name\": run_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"results\": results,\n",
    "                \"config\": config,\n",
    "                \"buckets\": buckets,\n",
    "                \"index\": index,\n",
    "            }\n",
    "\n",
    "            # If no preloaded scores exist  \n",
    "            if (all_scores_df is None or recalculate is True) and (score_results is True or rank_encoders is True): \n",
    "                scores_df = score(run=run)\n",
    "                scores_dfs.append(scores_df)\n",
    "\n",
    "            # Only save to runs if plotting results otherwise it is wasting memory\n",
    "            if plot_results:\n",
    "                runs.append(run)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load: {run_name}\")\n",
    "            print(e)\n",
    "\n",
    "# Save the scores dataframe to disk if it has calculated\n",
    "if len(scores_dfs) != 0:\n",
    "    all_scores_df = pd.concat(scores_dfs, ignore_index=True)\n",
    "    os.makedirs(score_path, exist_ok=True)\n",
    "    all_scores_df.to_pickle(score_path + score_file)\n",
    "\n",
    "\n",
    "print(len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_runtime != 0:\n",
    "    output = f\"Total runtime (multiple scales): \\n{round(total_runtime / 3600, 2)} hours \\n{round(total_runtime / 60, 2)} minutes \\n{round(total_runtime, 2)} seconds\"\n",
    "\n",
    "    plot_path = f\"plots\\\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    file_path = f\"{plot_path}\\\\total_runtime.txt\"\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scores_df.shape)\n",
    "all_scores_df = all_scores_df.drop(columns=[\"run_name\", \"timestamp\", \"index\", \"numerical_encoding\"])\n",
    "all_scores_df[\"buckets\"] = all_scores_df[\"buckets\"].astype(str)\n",
    "print(all_scores_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_results:\n",
    "    results_config = [\n",
    "        ('result_DAE_trace_Order', 'labels_DAE_trace', Perspective.ORDER, 'trace'),\n",
    "        ('result_DAE_trace_Attribute', 'labels_DAE_trace', Perspective.ATTRIBUTE, 'trace'),\n",
    "        ('result_DAE_trace_Arrival Time', 'labels_DAE_trace', Perspective.ARRIVAL_TIME, 'trace'),\n",
    "        ('result_DAE_trace_Workload', 'labels_DAE_trace', Perspective.WORKLOAD, 'trace'),\n",
    "    ]\n",
    "\n",
    "    nr_buckets = 0\n",
    "    for run in runs:\n",
    "        if run[\"buckets\"] is None:\n",
    "            nr_buckets += 1\n",
    "        else:\n",
    "            nr_buckets += len(run[\"buckets\"])\n",
    "\n",
    "    total_iterations = nr_buckets * len(results_config)\n",
    "    with tqdm(total=total_iterations, desc=\"Generating Plots\") as pbar:\n",
    "        for run in runs:\n",
    "            # print(f\"Generating: {directory}\\t{run_name}\")\n",
    "            for config in results_config:\n",
    "                # try:\n",
    "                bucket_plot_losses(\n",
    "                    results_name=config[0], \n",
    "                    labels_name=config[1],\n",
    "                    directory=directory,\n",
    "                    run_name=run[\"name\"],\n",
    "                    bucket_lengths=run[\"buckets\"],\n",
    "                    results=run[\"results\"],\n",
    "                    perspective=config[2],\n",
    "                    level=config[3],\n",
    "                    zoom=None,\n",
    "                    show_plots=show_plots,\n",
    "                    pbar=pbar)\n",
    "                # except:\n",
    "                #     print(\"Error loading \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the encoding methods combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"perspective\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "\n",
    "averages_methods = (\n",
    "    all_scores_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        \"roc_auc\": \"mean\", \n",
    "        \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", #[\"mean\", \"std\"],\n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\", #[\"mean\", \"std\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_methods.columns = ['_'.join(col).strip('_') for col in averages_methods.columns]\n",
    "averages_methods = averages_methods.reset_index()\n",
    "\n",
    "averages_methods[\"categorical_encoding\"] = \"All\"\n",
    "# averages_methods[\"timestamp\"] = \"Average\"\n",
    "\n",
    "result_methods_df = pd.concat([all_scores_df, averages_methods], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_methods.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scores_df.shape)\n",
    "print(averages_methods.shape)\n",
    "print(result_methods_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the perspectives combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"categorical_encoding\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "averages_perspective = (\n",
    "    result_methods_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        \"roc_auc\": \"mean\", \n",
    "        \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", \n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_perspective.columns = ['_'.join(col).strip('_') for col in averages_perspective.columns]\n",
    "averages_perspective = averages_perspective.reset_index()\n",
    "\n",
    "averages_perspective[\"perspective\"] = \"All\"\n",
    "\n",
    "\n",
    "result_df = pd.concat([result_methods_df, averages_perspective], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_methods_df.shape)\n",
    "print(averages_perspective.shape)\n",
    "print(result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional columns used in analysis\n",
    "result_df[\"dataset_size\"] = result_df[\"dataset\"].str.split('_').str[3]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"dataset\"].str.split('_').str[4]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"anomaly_percentage\"].astype(float)\n",
    "result_df[\"batch_size\"] = result_df[\"batch_size\"].astype(str)\n",
    "result_df[\"vector_size\"] = result_df[\"vector_size\"].astype(str)\n",
    "result_df[\"vector_window_size\"] = result_df[\"vector_size\"].astype(str) + '/' + result_df[\"window_size\"].astype(str)\n",
    "result_df[\"prefix\"] = result_df[\"prefix\"].astype(str)\n",
    "result_df[\"prefix_buckets\"] = result_df[\"prefix\"].astype(str) + '/' + result_df[\"buckets\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Experiment_Anomaly_Percentage\" in directory:\n",
    "    xlabel_name=\"Anomaly Percentages\"\n",
    "    label_name=\"anomaly_percentage\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Synthetic_Dataset\" in directory:\n",
    "    xlabel_name=\"Dataset Sizes\"\n",
    "    label_name=\"dataset_size\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Batch_Size\" in directory:\n",
    "    xlabel_name=\"Batch Sizes\"\n",
    "    label_name=\"batch_size\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "elif \"Experiment_Prefix_v2\" in directory:\n",
    "    xlabel_name=\"Prefix/Buckets\"\n",
    "    label_name=\"prefix_buckets\"\n",
    "    plot_scores(result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(result_df[result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "   \n",
    "elif \"Experiment_Finetuning_Fixed_Vector_Vector_Sizes\" in directory:\n",
    "    filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Fixed Vector\"]\n",
    "\n",
    "    xlabel_name=\"Vector Sizes\"\n",
    "    label_name=\"vector_size\"\n",
    "    plot_scores(filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  \n",
    "\n",
    "elif \"Experiment_Finetuning_T2V_Window_Vector_Sizes\" in directory:\n",
    "    atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Average Then Concatinate\"]\n",
    "    c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Concatinate\"]\n",
    "\n",
    "    xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "    label_name=\"vector_window_size\"\n",
    "    plot_scores(atc_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    xlabel_name=\"Vector/Window Sizes C\"\n",
    "    plot_scores(c_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "    plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')      \n",
    "      \n",
    "elif \"Experiment_Finetuning_W2V_Window_Vector_Sizes\" in directory:\n",
    "    atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Average Then Concatinate\"]\n",
    "    c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Concatinate\"]\n",
    "\n",
    "    xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "    label_name=\"vector_window_size\"\n",
    "    plot_scores(atc_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "    xlabel_name=\"Vector/Window Sizes C\"\n",
    "    plot_scores(c_filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "    plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')  \n",
    "\n",
    "elif \"Experiment_Real_World_All_Models\" in directory:\n",
    "    filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "    print(filtered_result_df[\"categorical_encoding\"].unique())\n",
    "\n",
    "    xlabel_name=\"Categorical Encoding\"\n",
    "    label_name=\"categorical_encoding\"\n",
    "    plot_scores(filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  \n",
    "\n",
    "\n",
    "elif \"Experiment_Synthetic_All_Models\" in directory:\n",
    "    filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "    print(filtered_result_df[\"categorical_encoding\"].unique())\n",
    "\n",
    "    xlabel_name=\"Categorical Encoding\"\n",
    "    label_name=\"categorical_encoding\"\n",
    "    plot_scores(filtered_result_df, directory, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"All\"], directory, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df[\"dataset_short_name\"] = all_scores_df[\"dataset\"].str.split('_').str[3] + \"/\" + all_scores_df[\"dataset\"].str.split('_').str[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_levels = (\n",
    "    all_scores_df.groupby(['dataset_short_name', 'categorical_encoding', 'level'])[['f1']]\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_combined_levels = (\n",
    "    all_scores_df.groupby(['dataset_short_name', 'categorical_encoding'])[['f1']]\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_combined_levels['level'] = 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_combined_levels['index'] = [None] \n",
    "grouped_levels = pd.concat([grouped_all_levels, grouped_combined_levels], axis=0, ignore_index=True)\n",
    "grouped_levels.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in grouped_levels.columns]\n",
    "grouped_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels['f1_rank'] = grouped_levels.groupby(['dataset_short_name', 'level'])['f1_mean'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "grouped_levels.to_csv(f'{plot_path}\\grouped_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "g = sns.FacetGrid(grouped_levels, col=\"level\", sharey=True, sharex=False, height=5, aspect=1.5, col_wrap=2)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"dataset_short_name\",\n",
    "    y=\"f1_rank\",\n",
    "    hue=\"categorical_encoding\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Dataset\", \"Rank (1 = Best)\")\n",
    "g.set_titles(\"Level: {col_name}\")\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.figure.suptitle(f\"{directory}: Ranking of Categorical Encodings methods per Dataset and Level\", fontsize=16)\n",
    "\n",
    "# for ax in g.axes.flatten():\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "#     ax.grid(False)\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    # Get the current tick positions and labels\n",
    "    tick_positions = ax.get_xticks()\n",
    "    tick_labels = ax.get_xticklabels()\n",
    "\n",
    "    # Set the ticks and labels explicitly with rotation\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.grid(False)  # Turn off gridlines\n",
    "    \n",
    "g.add_legend(title=\"Categorical Encoding\", loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "plt.savefig(f\"{plot_path}\\encoding_ranking_per_dataset.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_stats_df = (\n",
    "    grouped_levels.groupby(['categorical_encoding', 'level'])\n",
    "    .agg(\n",
    "        rank_mean=('f1_rank', 'mean'),\n",
    "        rank_std=('f1_rank', 'std'),\n",
    "        f1_mean=('f1_mean', 'mean'),\n",
    "        f1_std=('f1_mean', 'std')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rank_stats_df['rank_std'] = rank_stats_df['rank_std'].fillna(0)\n",
    "rank_stats_df['f1_std'] = rank_stats_df['f1_std'].fillna(0)\n",
    "\n",
    "rank_stats_df['rank_mean'] = rank_stats_df['rank_mean'].round(2)\n",
    "rank_stats_df['rank_std'] = rank_stats_df['rank_std'].round(2)\n",
    "rank_stats_df['f1_mean'] = rank_stats_df['f1_mean'].round(2)\n",
    "rank_stats_df['f1_std'] = rank_stats_df['f1_std'].round(2)\n",
    "\n",
    "rank_stats_df = rank_stats_df.sort_values(by=['level', 'rank_mean'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"plots\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "rank_stats_df.to_csv(f'{plot_path}\\summarised_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_rankings(data_df, title, rank_column='rank_mean', error_column='rank_std', sharey=True):\n",
    "    levels = ['combined', 'trace', 'event', 'attribute']\n",
    "    fig, axes = plt.subplots(1, len(levels), figsize=(15, 4), sharey=sharey)\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        ax = axes[i]\n",
    "        level_data = data_df[data_df['level'] == level]\n",
    "        \n",
    "        level_data_sorted = level_data.sort_values(rank_column)\n",
    "        ordered_categories = level_data_sorted['categorical_encoding']\n",
    "        y_values = level_data_sorted[rank_column]\n",
    "        errors = level_data_sorted[error_column]\n",
    "\n",
    "        ax.bar(\n",
    "            ordered_categories, \n",
    "            y_values, \n",
    "            yerr=errors,\n",
    "            color='skyblue', \n",
    "            capsize=5,\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Level: {level}\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Average Rank (Lower = Better)\")\n",
    "        ax.tick_params(axis=\"x\", rotation=90)\n",
    "        ax.grid(False)\n",
    "        ax.grid(axis='y', linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    fig.suptitle(f\"{title}: Average {rank_column} with Uncertainty Margins per Level\", fontsize=16)\n",
    "\n",
    "    plot_path = f\"plots/{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    plt.savefig(f\"{plot_path}/encoding_rankings_summary_{rank_column}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rankings(rank_stats_df, title='Rank', rank_column='rank_mean', error_column='rank_std', sharey=True)\n",
    "plot_rankings(rank_stats_df, title='F1', rank_column='f1_mean', error_column='f1_std', sharey=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
