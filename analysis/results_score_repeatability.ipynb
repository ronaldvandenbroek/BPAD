{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..')\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.fs import RESULTS_RAW_DIR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_runs(n):\n",
    "    def is_numeric(input):\n",
    "        try:\n",
    "            float(input)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "    # Filter numeric folder names\n",
    "    numeric_folders = [folder for folder in os.listdir(RESULTS_RAW_DIR) if is_numeric(folder) and os.path.isdir(os.path.join(RESULTS_RAW_DIR, folder))]\n",
    "\n",
    "    if numeric_folders:\n",
    "        # Sort folders by their numeric value, in descending order\n",
    "        sorted_folders = sorted(numeric_folders, key=lambda x: float(x), reverse=True)\n",
    "        \n",
    "        # Get the latest n runs\n",
    "        latest_n_runs = sorted_folders[:n]\n",
    "        \n",
    "        print(f\"The latest {n} runs are: {latest_n_runs}\")\n",
    "        return latest_n_runs\n",
    "    else:\n",
    "        print(\"No runs found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_subfolders(experiment_name):\n",
    "    experiment_path = os.path.join(RESULTS_RAW_DIR, experiment_name)\n",
    "    # Get all subfolder names in the specified directory\n",
    "    run_names = [name for name in os.listdir(experiment_path) if os.path.isdir(os.path.join(experiment_path, name))]\n",
    "    return run_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(run_name, verbose=False,directory=None):\n",
    "    if directory:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, directory, run_name)\n",
    "    else:\n",
    "        run_path = os.path.join(RESULTS_RAW_DIR, run_name)\n",
    "    npy_files = [file for file in os.listdir(run_path) if file.endswith('.npy')]\n",
    "\n",
    "    loaded_data = {}\n",
    "\n",
    "    # Load each .npy file and use the file name (without extension) as the key\n",
    "    for npy_file in npy_files:\n",
    "        file_path = os.path.join(run_path, npy_file)\n",
    "        key = os.path.splitext(npy_file)[0]  # Get the file name without .npy extension\n",
    "        loaded_data[key] = np.load(file_path)\n",
    "\n",
    "        if verbose: print(f\"{loaded_data[key].shape} \\t {key}\")\n",
    "\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets(keys):\n",
    "    buckets = set()\n",
    "    for key in keys:\n",
    "        numbers = re.findall(r'\\d+', key)\n",
    "        buckets.update(map(int, numbers))\n",
    "    if len(buckets) > 0:\n",
    "        return sorted(buckets)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def filter(list, string):\n",
    "    return [item for item in list if string in item]\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    # Check if both precision and recall are zero to avoid division by zero\n",
    "    precision[precision == 0] = 1e-6\n",
    "    recall[recall == 0] = 1e-6\n",
    "\n",
    "    if np.any(precision != 0) and np.any(recall != 0):\n",
    "        return (2 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def _calculate_results(perspective_index, perspective, bucket, level, level_label_keys, level_result_keys, results, verbose):\n",
    "    if verbose: print(perspective_index, perspective)\n",
    "\n",
    "    # Get the key names\n",
    "    if bucket is not None:\n",
    "        true_key = filter(level_label_keys, f'_{bucket}')\n",
    "        pred_prob_key = filter(level_result_keys, f'{perspective}_{bucket}')\n",
    "    else:\n",
    "        true_key = level_label_keys\n",
    "        pred_prob_key = filter(level_result_keys, f'{perspective}')\n",
    "    if verbose: print(true_key, pred_prob_key)\n",
    "\n",
    "    # Check if the files exist by checking the length\n",
    "    if len(true_key) != 1 or len(pred_prob_key) != 1:\n",
    "        if verbose: print(f'Skipping as {perspective} has not been found.')\n",
    "        return None\n",
    "    else:\n",
    "        true_key = true_key[0]\n",
    "        pred_prob_key = pred_prob_key[0]\n",
    "        if verbose: print(true_key, pred_prob_key)\n",
    "\n",
    "    # Get the values by key\n",
    "    pred_prob = np.array(results[pred_prob_key],dtype=np.float32)\n",
    "    if level == 'trace':\n",
    "        y_true = np.array(results[true_key][:,perspective_index],dtype=np.float32)\n",
    "    elif level == 'event':\n",
    "        # Also need to take the bucket_size into account\n",
    "        if bucket is None:\n",
    "            y_true = np.array(results[true_key][:,:,perspective_index],dtype=np.float32)\n",
    "        else:\n",
    "            y_true = np.array(results[true_key][:,:bucket,perspective_index],dtype=np.float32)\n",
    "    else: #attribute\n",
    "        if bucket is None:\n",
    "            y_true = np.array(results[true_key][:,:,:,perspective_index],dtype=np.float32)\n",
    "        else:\n",
    "            y_true = np.array(results[true_key][:,:bucket,:,perspective_index],dtype=np.float32)\n",
    "\n",
    "    if verbose: print(y_true.shape, pred_prob.shape)\n",
    "\n",
    "    y_true = y_true.flatten()\n",
    "    pred_prob = pred_prob.flatten()\n",
    "\n",
    "    # Check if all y_true values are 0 (most likely indicates a problem with an earlier step)\n",
    "    if np.all(y_true == 0):\n",
    "        if verbose: print(f'No positive class found for {true_key},{pred_prob_key}')\n",
    "        return None\n",
    "\n",
    "    # print(true, len(true))\n",
    "    # print(pred_prob, len(pred_prob))\n",
    "    if verbose: print(y_true.shape, pred_prob.shape)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true=y_true, probas_pred=pred_prob)\n",
    "    # print(precision.shape)\n",
    "    # print(recall.shape)\n",
    "    f1s=calculate_f1(precision,recall)\n",
    "    f1s[np.isnan(f1s)] = 0\n",
    "    # print(f1s)\n",
    "    best_index=np.argmax(f1s)\n",
    "    # if True: print(best_index, len(f1s), f1s[best_index])\n",
    "    return (pred_prob_key,f1s[best_index],thresholds[best_index])\n",
    "\n",
    "def calculate_results(results, level, verbose=False):\n",
    "    level_file_keys = filter(results.keys(), level)\n",
    "    level_label_keys = filter(level_file_keys, 'label')\n",
    "    level_result_keys = filter(level_file_keys, 'result')\n",
    "    bucket_lengths = get_buckets(level_file_keys)\n",
    "\n",
    "    if verbose: print(results.keys())\n",
    "\n",
    "    f1_scores = []\n",
    "    for perspective_index, perspective in enumerate(Perspective.values()):\n",
    "        if bucket_lengths is None:\n",
    "            f1_score = _calculate_results(perspective_index, perspective, None, level, level_label_keys, level_result_keys, results, verbose)\n",
    "            if f1_score is not None:\n",
    "                f1_scores.append(f1_score)\n",
    "        else:\n",
    "            for bucket in bucket_lengths:\n",
    "                f1_score = _calculate_results(perspective_index, perspective, bucket, level, level_label_keys, level_result_keys, results, verbose)\n",
    "                if f1_score is not None:\n",
    "                    f1_scores.append(f1_score)\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(title, label_list, results_lists, legend=True):\n",
    "    # Prepare the data for multiple lists\n",
    "    all_names = set()\n",
    "    for trace_results in results_lists:\n",
    "        all_names.update([item[0] for item in trace_results])\n",
    "    \n",
    "    all_names = sorted(all_names)  # Sort names to maintain consistency in bar positions\n",
    "    name_indices = np.arange(len(all_names))  # Create an index for each name\n",
    "    \n",
    "    # Initialize plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    # bar_width = 0.2  # Set the width of each bar\n",
    "    offset = 0  # To shift bars for different lists\n",
    "    num_lists = len(results_lists)\n",
    "    bar_width = 0.9 / num_lists  # Reduce bar width based on the number of lists\n",
    "    # offset = -(bar_width * num_lists) / 2  # Start offset for centering the bars\n",
    "    \n",
    "    # Plot bars for each result list\n",
    "    for i, (label, trace_results) in enumerate(zip(label_list, results_lists)):\n",
    "        names = [item[0] for item in trace_results]\n",
    "        values = [item[1] for item in trace_results]\n",
    "        \n",
    "        # Align the values with all possible names, filling in 0 for missing values\n",
    "        aligned_values = [values[names.index(name)] if name in names else 0 for name in all_names]\n",
    "        \n",
    "        # Create the bar chart with offset\n",
    "        plt.barh(name_indices + offset, aligned_values, bar_width, label=label)\n",
    "        offset += bar_width  # Update the offset for the next set of bars\n",
    "    \n",
    "    # Set labels, ticks, and title\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.ylabel('Trace Results')\n",
    "    plt.yticks(name_indices + bar_width / 2, all_names)  # Center ticks between bars\n",
    "    plt.title(f'Bar Chart of {title}')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_f1_scores_simple(run_list, name_list, title):\n",
    "    num_bar_range = range(len(run_list))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    bar_colors = [colors[i // 10 % len(colors)] for i in num_bar_range]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(num_bar_range, run_list, color=bar_colors)\n",
    "\n",
    "    plt.xticks(num_bar_range, name_list, rotation=90) \n",
    "\n",
    "    plt.xlabel(\"Configurations\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(f'Bar Chart of {title}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_run(run_list,name_list,title,verbose=False,directory=None,legend=True):\n",
    "    results_traces = []\n",
    "    results_events = []\n",
    "    results_attributes = []\n",
    "    for run in run_list:\n",
    "        results = load_results(run_name=run, verbose=verbose,directory=directory)\n",
    "\n",
    "        results_traces.append(calculate_results(results, 'trace', verbose=verbose))\n",
    "        results_events.append(calculate_results(results, 'event', verbose=verbose))\n",
    "        #results_traces.append(f1_scores_attributes = calculate_results(results, 'attribute', verbose=True))\n",
    "\n",
    "    results_traces = np.array(results_traces)[:,:,1]\n",
    "    results_events = np.array(results_events)[:,:,1]\n",
    "\n",
    "    results_traces = np.array(results_traces, dtype=float)\n",
    "    results_events = np.array(results_events, dtype=float)\n",
    "\n",
    "    results_traces = np.mean(results_traces, axis=1)\n",
    "    results_events = np.mean(results_events, axis=1)\n",
    "\n",
    "    print(results_traces.shape)\n",
    "    print(results_events.shape)\n",
    "\n",
    "    plot_f1_scores_simple(results_traces,name_list,title)\n",
    "    plot_f1_scores_simple(results_events,name_list,title)\n",
    "\n",
    "    # plot_f1_scores(title=f'Trace Level - {title}', label_list=name_list, results_lists=results_traces, legend=legend)\n",
    "    # plot_f1_scores(title=f'Event Level - {title}', label_list=name_list, results_lists=results_events, legend=legend)\n",
    "    # plot_f1_scores(title=f'Attribute Level -  {title}', name_list=name_list, results_lists=results_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"DAE_Repeatability\" \n",
    "run_list = list_subfolders(directory)\n",
    "print(run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "repeats = 10\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    name_list.append(f\"W2V_{repeat}\")\n",
    "for repeat in range(repeats):\n",
    "    name_list.append(f\"ONE_HOT_{repeat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 1 no workload and arrival-time\n",
    "analyze_run(run_list=run_list,name_list=name_list,title=directory,directory=directory,legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
