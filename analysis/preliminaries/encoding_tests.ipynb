{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Word2Vec encoding limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# RCVDB: Dictionary only needs to keep track of the unique values of a single attribute\n",
    "class AttributeDictionary():\n",
    "    def __init__(self, start_index=0, max_size=100) -> None:\n",
    "        self.max_size = max_size\n",
    "        self.start_index = start_index\n",
    "        self.encodings = defaultdict(str)\n",
    "        self.encodings_inv = defaultdict(int)\n",
    "\n",
    "    def encode(self, label):\n",
    "        if label in self.encodings.keys():\n",
    "            return str(self.encodings[label])\n",
    "        else:\n",
    "            existing_keys = self.encodings_inv.keys()\n",
    "            new_key = len(existing_keys) + self.start_index\n",
    "            self.encodings[label] = new_key\n",
    "            self.encodings_inv[new_key] = label\n",
    "            return str(new_key)\n",
    "        \n",
    "    def decode(self, value):\n",
    "        value = int(value)\n",
    "        if value in self.encodings_inv.keys():\n",
    "            return self.encodings_inv[value]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def encoded_attributes(self):\n",
    "        return [str(i) for i in map(str, self.encodings_inv.keys())]\n",
    "    \n",
    "    # Buffer attributes are all encoded labels that are not part of a label_value mapping\n",
    "    def buffer_attributes(self):\n",
    "        current_size = len(self.encodings_inv.keys())\n",
    "        return [str(i) for i in range(current_size + self.start_index, self.max_size + self.start_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"gray\", \"black\"]\n",
    "unknown_attributes = [\"cyan\", \"magenta\", \"teal\", \"lavender\", \"beige\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrDict = AttributeDictionary(max_size=100)\n",
    "for att in all_attributes:\n",
    "    attrDict.encode(att)\n",
    "\n",
    "print(all_attributes)\n",
    "\n",
    "encoded_attributes = attrDict.encoded_attributes()\n",
    "buffer_attributes = attrDict.buffer_attributes()\n",
    "\n",
    "print(encoded_attributes)\n",
    "\n",
    "decoded_attributes = []\n",
    "for val in encoded_attributes:\n",
    "    decoded_attributes.append(attrDict.decode(val[0]))\n",
    "\n",
    "print(decoded_attributes)\n",
    "\n",
    "print(encoded_attributes + buffer_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def convert_to_sentences(input):\n",
    "    return [[str(i)] for i in input]\n",
    "\n",
    "buffered_w2v_model = Word2Vec(sentences=convert_to_sentences(encoded_attributes + buffer_attributes), vector_size=15, window=5, min_count=1)\n",
    "unbuffered_w2v_model = Word2Vec(sentences=convert_to_sentences(encoded_attributes), vector_size=15, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the known words\n",
    "buff_w2v_encodings = []\n",
    "unbuff_w2v_encodings = []\n",
    "for attr in encoded_attributes:\n",
    "    buff_w2v_encodings.append(buffered_w2v_model.wv[attr])\n",
    "    unbuff_w2v_encodings.append(unbuffered_w2v_model.wv[attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_attribute = unknown_attributes[0]\n",
    "enc_unkown_attribute = attrDict.encode(unknown_attribute)\n",
    "\n",
    "print(unknown_attribute, enc_unkown_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cant deal with unknown values\n",
    "try:\n",
    "    unbuffered_w2v_model.wv[enc_unkown_attribute]\n",
    "except KeyError as e:\n",
    "    print(e, 'in the unbuffered w2v')\n",
    "try:\n",
    "    buffered_w2v_model.wv[unknown_attribute]\n",
    "except KeyError as e: \n",
    "    print(e, 'in the buffered (and unbufferd) w2v')\n",
    "\n",
    "# But can deal with the encoded unknown value if buffered\n",
    "try:\n",
    "    w2v_unknown_attribute = buffered_w2v_model.wv[enc_unkown_attribute]\n",
    "    print(enc_unkown_attribute, 'is present in the buffered w2v')\n",
    "    print(w2v_unknown_attribute)\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "\n",
    "# Find the most similar words to the unknown vector\n",
    "similar_words = buffered_w2v_model.wv.similar_by_vector(w2v_unknown_attribute)\n",
    "\n",
    "# Note that the 10 (cyan) has a 100% match while it is not in the original dataset\n",
    "print(similar_words)\n",
    "\n",
    "best_match = similar_words[0][0]\n",
    "print(attrDict.decode(best_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder has the same problem\n",
    "Thus a dictionary keeping track of the attributes is also needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "all_attributes_encoded = encoder.fit_transform(all_attributes)\n",
    "\n",
    "print(all_attributes_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cant deal with unknown values\n",
    "try:\n",
    "    encoder.transform(unknown_attributes)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class ProcessWord2Vec():\n",
    "    def __init__(self, training_sentances, vector_size=50, window=5, min_count=1, workers=4, attr_dicts=None, debug=False) -> None:\n",
    "        self.debug = debug\n",
    "        self.attr_dicts = attr_dicts\n",
    "\n",
    "        if self.attr_dicts is not None:\n",
    "            training_sentances = self._encode_training_sentences(copy.deepcopy(training_sentances))\n",
    "\n",
    "        self.w2v_model = Word2Vec(sentences=training_sentances, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "\n",
    "    def _encode_training_sentences(self, training_sentances):\n",
    "        if self.debug: print(f\"Training Sentances: {training_sentances}\")\n",
    "\n",
    "        for i, sentance in enumerate(training_sentances):\n",
    "            for j, word in enumerate(sentance):\n",
    "                encoded_word = self.attr_dicts[j].encode(word)\n",
    "                training_sentances[i][j] = encoded_word\n",
    "\n",
    "        for attr_dict in self.attr_dicts:\n",
    "            attr_dict:AttributeDictionary\n",
    "            training_sentances += ProcessWord2Vec.convert_to_sentences(attr_dict.buffer_attributes())\n",
    "        training_sentances\n",
    "\n",
    "        if self.debug: print(f\"Encoded Training Sentances: {training_sentances}\")\n",
    "        return training_sentances         \n",
    "\n",
    "    # Function to get the Word2Vec vector for an attribute\n",
    "    def _get_attr_vector(self, attr_index, attr):\n",
    "        if self.attr_dicts is not None:\n",
    "            mapped_attr = self.attr_dicts[attr_index].encode(attr)\n",
    "            \n",
    "            if self.debug: print(f\"\\t\\tMapped {attr} to {mapped_attr}\")\n",
    "        else:\n",
    "            mapped_attr = attr\n",
    "        return self.w2v_model.wv[mapped_attr]\n",
    "\n",
    "    # Function to encode an event by averaging its attribute vectors\n",
    "    def _encode_event(self, event):\n",
    "        if self.debug: print(f\"\\tEncoding Event: {event}\")\n",
    "        attribute_vectors = np.array([self._get_attr_vector(i, attr) for i, attr in enumerate(event)])\n",
    "        event_vector = np.mean(attribute_vectors, axis=0)\n",
    "        return event_vector\n",
    "\n",
    "    # Function to encode a trace by concatenating its event vectors\n",
    "    def encode_trace(self, trace):\n",
    "        if self.debug: print(f\"Encoding Trace: {trace}\")\n",
    "        event_vectors = [self._encode_event(event) for event in trace]\n",
    "        trace_vector = np.concatenate(event_vectors, axis=0)\n",
    "        return trace_vector      \n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_sentences(input):\n",
    "        return [[str(i)] for i in input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_attributes = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"gray\", \"black\"]\n",
    "\n",
    "# Create traces with random selection of attributes\n",
    "length = 10\n",
    "size = 100\n",
    "attribute_traces = [random.sample(all_attributes, length) for _ in range(size)]\n",
    "unique_attributes = list(set(attribute for sublist in attribute_traces for attribute in sublist))\n",
    "print(attribute_traces)\n",
    "\n",
    "# Convert the traces labels to integers\n",
    "attrDict = AttributeDictionary(max_size=100)\n",
    "for i, trace in enumerate(attribute_traces):\n",
    "    for j, attribute in enumerate(trace):\n",
    "        attribute_traces[i][j] = attrDict.encode(attribute)\n",
    "print(attribute_traces)\n",
    "\n",
    "# Append the buffer to the traces\n",
    "# If some pre-existing data is available it can be used to improve the w2v training, however, in theory no pre-training is nessessary.\n",
    "# In the latter case only the buffer is used exclusively\n",
    "def convert_to_sentences(input):\n",
    "    return [[str(i)] for i in input]\n",
    "sentances_pre_training = attribute_traces + convert_to_sentences(attrDict.buffer_attributes())\n",
    "print(sentances_pre_training)\n",
    "\n",
    "no_pretrain_attrDict = AttributeDictionary(max_size=100)\n",
    "scentences_only_buffer = convert_to_sentences(no_pretrain_attrDict.buffer_attributes())\n",
    "print(scentences_only_buffer)\n",
    "\n",
    "# The w2v model can then be trained on any existing traces and the generic buffer\n",
    "process_w2v_model = ProcessWord2Vec(training_sentances=sentances_pre_training)\n",
    "process_w2v_model_only_buffer = ProcessWord2Vec(training_sentances=scentences_only_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reduce_embedding_space(w2v_model):\n",
    "    words = w2v_model.wv.index_to_key\n",
    "    word_vectors = np.array([w2v_model.wv[word] for word in words])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "    # tsne = TSNE(n_components=2, random_state=2024)\n",
    "    # word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    return words, word_vectors_2d\n",
    "\n",
    "words, word_vectors_2d = reduce_embedding_space(process_w2v_model.w2v_model)\n",
    "words_only_buffer, word_vectors_2d_only_buffer = reduce_embedding_space(process_w2v_model_only_buffer.w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each category with different colors\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], color='red', label='With Pre-Train Traces')\n",
    "plt.scatter(word_vectors_2d_only_buffer[:, 0], word_vectors_2d_only_buffer[:, 1], color='blue', label='Only Buffer')\n",
    "\n",
    "# Annotate the points with the corresponding words\n",
    "for i, word in enumerate(words):\n",
    "    if int(word) < 10:\n",
    "        plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), fontsize=12, color='green')\n",
    "\n",
    "for i, word in enumerate(words_only_buffer):\n",
    "    if int(word) < 10:\n",
    "        plt.annotate(word, xy=(word_vectors_2d_only_buffer[i, 0], word_vectors_2d_only_buffer[i, 1]), fontsize=12, color='black')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Word2Vec Word Embeddings Visualization')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Wrapper Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_color_attributes = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"gray\", \"black\"]\n",
    "all_shape_attributes = [\"circle\", \"square\", \"triangle\", \"rectangle\", \"oval\", \"hexagon\", \"star\", \"pentagon\", \"rhombus\", \"octagon\"]\n",
    "unknown_color_attributes = [\"cyan\", \"magenta\", \"teal\", \"lavender\", \"beige\"]\n",
    "unknown_shape_attributes = [\"trapezoid\", \"parallelogram\", \"crescent\", \"heart\", \"kite\"]\n",
    "\n",
    "\n",
    "length = 3\n",
    "size = 5\n",
    "\n",
    "color_traces = [random.sample(all_color_attributes, length) for _ in range(size)]\n",
    "shape_traces = [random.sample(all_shape_attributes, length) for _ in range(size)]\n",
    "combined_traces = [[list(pair) for pair in zip(shape_list, color_list)] for shape_list, color_list in zip(shape_traces, color_traces)]\n",
    "flattened_combined_traces = [pair for sublist in combined_traces for pair in sublist]\n",
    "\n",
    "unknown_color_traces = [random.sample(unknown_color_attributes, length) for _ in range(size)]\n",
    "unknown_shape_traces = [random.sample(unknown_shape_attributes, length) for _ in range(size)]\n",
    "unknown_combined_traces = [[list(pair) for pair in zip(shape_list, color_list)] for shape_list, color_list in zip(unknown_shape_traces, unknown_color_traces)]\n",
    "\n",
    "print(combined_traces)\n",
    "print(flattened_combined_traces)\n",
    "print(unknown_combined_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_attr_dict = AttributeDictionary(max_size=20)\n",
    "shape_attr_dict = AttributeDictionary(start_index=color_attr_dict.max_size, max_size=20)\n",
    "wrapped_process_w2v_model = ProcessWord2Vec(training_sentances=flattened_combined_traces, attr_dicts=[color_attr_dict, shape_attr_dict], debug=True)\n",
    "\n",
    "print(combined_traces)\n",
    "print(flattened_combined_traces)\n",
    "\n",
    "encoded_traces = []\n",
    "for trace in unknown_combined_traces:\n",
    "    encoded_traces.append(wrapped_process_w2v_model.encode_trace(trace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_vectors_2d = reduce_embedding_space(wrapped_process_w2v_model.w2v_model)\n",
    "\n",
    "# Plot each category with different colors\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], color='red', label='With Pre-Train Traces')\n",
    "\n",
    "# Annotate the points with the corresponding words\n",
    "for i, word in enumerate(words):\n",
    "    if int(word) < 8:\n",
    "        plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), fontsize=12, color='black')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Word2Vec Word Embeddings Visualization')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
