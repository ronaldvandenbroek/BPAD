{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.abspath('..\\..')\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from utils.enums import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results, labels, directory, experiment_name, run_name, perspective, level, bucket=None, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True):\n",
    "    def scatter_plot(ax, results, labels):\n",
    "        y_values = results\n",
    "        x_values = np.arange(len(results))\n",
    "        ax.scatter(x_values[labels == 0], y_values[labels == 0], c='grey', s=3, label='Normal Prefixes', zorder=1)\n",
    "        ax.scatter(x_values[labels == 1], y_values[labels == 1], c='red', s=3, label='Anomalous Prefixes', zorder=2)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Normalize results\n",
    "    results = np.interp(results, (results.min(), results.max()), (0, 1))\n",
    "\n",
    "    subtitle = f'{name}     {run_name}'\n",
    "    if len(results) == 0:\n",
    "        print(f'ERROR no results found for {subtitle}')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        labels = labels[:, perspective]\n",
    "        scatter_plot(ax, results, labels)\n",
    "        \n",
    "        perspective_name = Perspective.values()[perspective]\n",
    "\n",
    "        bucket_string = ''\n",
    "        if bucket is not None:\n",
    "            bucket_string = f'with bucket size {str(bucket)}'\n",
    "        \n",
    "        title = f'Error per Prefix on the {perspective_name} perspective at {level} level {bucket_string}'\n",
    "        \n",
    "        # Print to keep track of plotting\n",
    "        # print(f'\\t {title}')\n",
    "        \n",
    "        plt.title(f'{title}\\n{subtitle}')\n",
    "        plt.xlabel('Prefix Index')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        if zoom:\n",
    "            axins = inset_axes(ax, width=\"60%\", height=\"60%\", loc='upper right')\n",
    "\n",
    "            scatter_plot(axins, results, labels)\n",
    "            axins.set_xlim(zoom[0])\n",
    "            axins.set_ylim(zoom[1])\n",
    "            _,_ = ax.indicate_inset_zoom(axins, edgecolor=\"black\", linewidth=3)\n",
    "\n",
    "        plt.xlabel('Case Index')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        plot_path = f\"results\\{directory}\\{run_name}\\error_plots\"\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        plt.savefig(f\"{plot_path}\\{perspective_name}_{level}_{bucket_string}.png\", format='png', dpi=300)\n",
    "        \n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def bucket_plot_losses(results_name, labels_name, run_name, directory, name, bucket_lengths, results, perspective, level, zoom=[[11000,13000],[-0.05, 0.2]], show_plots=True, pbar=None):\n",
    "    if bucket_lengths is None:\n",
    "        plot_losses(\n",
    "            results=results[f'{results_name}'], \n",
    "            labels=results[f'{labels_name}'],\n",
    "            directory=directory,\n",
    "            experiment_name=experiment_name,\n",
    "            run_name=run_name, perspective=perspective, level=level, bucket=None, zoom=zoom, show_plots=show_plots)\n",
    "        if pbar:\n",
    "            pbar.update(1)       \n",
    "    else:\n",
    "        for bucket in bucket_lengths:\n",
    "            plot_losses(\n",
    "                results=results[f'{results_name}_{bucket}'], \n",
    "                labels=results[f'{labels_name}_{bucket}'],\n",
    "                directory=directory,\n",
    "                experiment_name=experiment_name,\n",
    "                run_name=run_name, perspective=perspective, level=level, bucket=bucket, zoom=zoom, show_plots=show_plots)\n",
    "            if pbar:\n",
    "                pbar.update(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_single_score(data, score_type, level, ax, label_name, xlabel_name):\n",
    "    subset = data[data[\"level\"] == level]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=label_name,\n",
    "        y=score_type,\n",
    "        hue=\"perspective\",\n",
    "        # style=\"categorical_encoding\",\n",
    "        markers=True,\n",
    "        dashes=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{score_type.capitalize()} Scores (Level={level.capitalize()})\")\n",
    "    ax.set_xlabel(xlabel_name)\n",
    "    ax.set_ylabel(f\"{score_type.capitalize()}\" if level=='trace' else \"\")\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "def plot_scores(data, directory, experiment_name, title, label_name, xlabel_name, summary=True, filter_beginning_percentage=0, postfix=None, legend_title=\"Anomaly Perspective & Encoding\", run_time=True):\n",
    "    levels = [\"trace\", \"event\", \"attribute\"]\n",
    "    \n",
    "    if summary:\n",
    "        if not run_time:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True, sharey=False)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 8), sharex=True, sharey=False)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(6, 3, figsize=(18, 18), sharex=True, sharey=False)\n",
    "    \n",
    "    handles, labels = [], []\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        if not run_time:\n",
    "            plot_single_score(data, \"f1\", level, axes[i], label_name, xlabel_name)\n",
    "        else:\n",
    "            plot_single_score(data, \"f1\", level, axes[0, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"run_time\", level, axes[1, i], label_name, xlabel_name)\n",
    "        if not summary:\n",
    "            plot_single_score(data, \"pr_auc\", level, axes[2, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"roc_auc\", level, axes[3, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"precision\", level, axes[4, i], label_name, xlabel_name)\n",
    "            plot_single_score(data, \"recall\", level, axes[5, i], label_name, xlabel_name)\n",
    "\n",
    "        if not handles and not labels:\n",
    "            if not run_time:\n",
    "                handles, labels = axes[i].get_legend_handles_labels()\n",
    "            else:\n",
    "                handles, labels = axes[0, i].get_legend_handles_labels()\n",
    "        \n",
    "        if not run_time:\n",
    "            axes[i].legend().remove()\n",
    "        else:\n",
    "            axes[0, i].legend().remove()\n",
    "            axes[1, i].legend().remove()\n",
    "        if not summary:\n",
    "            axes[2, i].legend().remove()\n",
    "            axes[3, i].legend().remove()\n",
    "            axes[4, i].legend().remove()\n",
    "            axes[5, i].legend().remove()\n",
    "\n",
    "\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=legend_title,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        ncol=3\n",
    "    )\n",
    "    \n",
    "    fig.suptitle(f'F1-Scores: {experiment_name}: {title}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    if run_time:\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "\n",
    "    plot_path = f\"results\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    if postfix is not None:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{postfix}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.savefig(f\"{plot_path}\\experimental_results_{title}_{filter_beginning_percentage}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "    # 'Transformer_Event_Multi_Task'\n",
    "    # 'Transformer_Perspective_Weights'\n",
    "    # 'Transformer_Event_Positional_Encoding'\n",
    "    # 'Transformer_Prefix_Store' \n",
    "    \n",
    "    # -- Experimental Setup --\n",
    "    # 'Bucketing',\n",
    "    # 'Batch_Size',\n",
    "    # 'Anomaly_Percentage',\n",
    "    # 'Synthetic_Dataset',\n",
    "\n",
    "    # -- Encoding Finetuning --\n",
    "    # 'Finetuning_F-V',\n",
    "    # 'Finetuning_T2V',\n",
    "    # 'Finetuning_W2V',\n",
    "\n",
    "    # -- Encoding Experiments --\n",
    "    'Synthetic_All_Models',\n",
    "    # 'Real_World_All_Models',\n",
    "    ]\n",
    "\n",
    "# Get the root experiment name from directory\n",
    "experiment_name = re.sub(r\"_\\d{5}$\", \"\", directories[0])\n",
    "# model_type = 'Transformer' # '{model_type}'\n",
    "model_type = 'PB-DAE'\n",
    "\n",
    "filter_beginning_percentage = 0\n",
    "\n",
    "recalculate = False\n",
    "\n",
    "score_results = True\n",
    "score_summary = True\n",
    "\n",
    "rank_encoders = True\n",
    "rank_cd_encoders = True\n",
    "\n",
    "plot_results = False\n",
    "show_plots = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = directories[-1]\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils.load_data import list_subfolders_or_zip_files\n",
    "\n",
    "\n",
    "run_list = list_subfolders_or_zip_files(directory)\n",
    "print(run_list)\n",
    "print(run_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils.load_data import load_score_dataframe\n",
    "\n",
    "score_path = f\"results\\{directory}\\\\\"\n",
    "score_file = f\"scores_raw_df_{filter_beginning_percentage}.pkl\"\n",
    "all_scores_df = None\n",
    "\n",
    "# Check if scores_raw_df.pkl exists and if yes skip reloading all data, force recalculation if set to true\n",
    "if not recalculate:\n",
    "    all_scores_df = load_score_dataframe(score_path + score_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed to merge two score dataframes\n",
    "# directory_2 = 'Experiment_Real_World_All_Models_T2V_C'\n",
    "# score_path_2 = f\"results\\{directory_2}\\\\\"\n",
    "\n",
    "# if not recalculate:\n",
    "#     all_scores_df_2 = load_score_dataframe(score_path_2 + score_file)\n",
    "# result = pd.concat([all_scores_df, all_scores_df_2], axis=0, ignore_index=True)\n",
    "\n",
    "# print(all_scores_df.shape)\n",
    "# print(all_scores_df_2.shape)\n",
    "# print(result.shape)\n",
    "\n",
    "# os.makedirs(score_path, exist_ok=True)\n",
    "# result.to_pickle(score_path + score_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils.load_data import cleanup_temp_folders, get_buckets, load_config, load_results, unzip_results\n",
    "from analysis.utils.process_raw_data import score\n",
    "\n",
    "\n",
    "runs = []\n",
    "scores_dfs = []\n",
    "total_runtime = 0\n",
    "\n",
    "# If no scores_raw_df.pkl exists, recalulate is true or plotting individual runs start loading all data from raw\n",
    "if all_scores_df is None or recalculate is True or plot_results is True:\n",
    "    for index, run_name in enumerate(tqdm(run_list)):\n",
    "        # try:\n",
    "            # If needed unzip the data\n",
    "            run_name, from_zip = unzip_results(directory, run_name)\n",
    "\n",
    "            # Loading the data\n",
    "            results = load_results(run_name=run_name, directory=directory)\n",
    "            config = load_config(run_name=run_name, directory=directory)\n",
    "            buckets = get_buckets(results.keys())\n",
    "            timestamp = run_name.split('_')[0]\n",
    "\n",
    "            # print(\"Buckets:\", buckets)\n",
    "\n",
    "            total_runtime += config['run_time']\n",
    "\n",
    "            # If needed clean up temp folder\n",
    "            if from_zip:\n",
    "                cleanup_temp_folders(directory, run_name)\n",
    "            \n",
    "            # If set filter the first % of results from the run to allow the scoring some grace period\n",
    "            if filter_beginning_percentage != 0:\n",
    "                for key, value in results.items():\n",
    "                    filter_index = int(value.shape[0] / filter_beginning_percentage)\n",
    "                    # print(filter_index)\n",
    "                    results[key] = value[filter_index:]\n",
    "\n",
    "\n",
    "            run = {\n",
    "                \"name\": run_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"results\": results,\n",
    "                \"config\": config,\n",
    "                \"buckets\": buckets,\n",
    "                \"index\": index,\n",
    "            }\n",
    "\n",
    "            # If no preloaded scores exist\n",
    "            if (all_scores_df is None or recalculate is True) and (score_results is True or rank_encoders is True or rank_cd_encoders is True): \n",
    "                try:\n",
    "                    scores_df = score(run=run)\n",
    "                    scores_dfs.append(scores_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to score: {run_name}\")\n",
    "                    print(e)\n",
    "\n",
    "            # Only save to runs if plotting results otherwise it is wasting memory\n",
    "            if plot_results:\n",
    "                runs.append(run)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Failed to load: {run_name}\")\n",
    "        #     print(e)\n",
    "\n",
    "print(len(scores_dfs))\n",
    "# Save the scores dataframe to disk if it has calculated\n",
    "if len(scores_dfs) != 0:\n",
    "    all_scores_df = pd.concat(scores_dfs, ignore_index=True)\n",
    "    os.makedirs(score_path, exist_ok=True)\n",
    "    all_scores_df.to_pickle(score_path + score_file)\n",
    "\n",
    "print(len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if recalculate == True:\n",
    "#     for i in range(0,4): \n",
    "#         labels = runs[0]['results']['labels_Transformer_trace'].T[i]\n",
    "#         print(labels.shape)\n",
    "#         true_count_y_true = np.sum(labels == 1)\n",
    "#         false_count_y_true = np.sum(labels == 0)\n",
    "#         # Anomalous, Normal\n",
    "#         print(true_count_y_true, false_count_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if recalculate == True:\n",
    "#     debug_order_trace_labels = runs[0]['results']['labels_Transformer_trace'].T[Perspective.ORDER]\n",
    "#     debug_order_event_labels = runs[0]['results']['labels_Transformer_event'].T[Perspective.ORDER].T\n",
    "#     debug_order_attribute_labels = runs[0]['results']['labels_Transformer_attribute'].T[Perspective.ORDER].T\n",
    "\n",
    "#     print(debug_order_attribute_labels.shape)\n",
    "#     print(debug_order_event_labels.shape)\n",
    "#     print(debug_order_trace_labels.shape)\n",
    "\n",
    "#     debug_order_trace_results = runs[0]['results']['result_Transformer_trace_Order']\n",
    "#     print(np.min(debug_order_trace_results), np.max(debug_order_trace_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recalculate == True:\n",
    "    scores_dfs[0]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_prefixes = np.array(config['processed_prefixes'])\n",
    "# processed_events = np.array(config['processed_events'])\n",
    "# print(processed_prefixes.shape)\n",
    "# print(processed_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_runtime != 0:\n",
    "    output = f\"Total runtime (multiple scales): \\n{round(total_runtime / 3600, 2)} hours \\n{round(total_runtime / 60, 2)} minutes \\n{round(total_runtime, 2)} seconds\"\n",
    "\n",
    "    plot_path = f\"results\\\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    file_path = f\"{plot_path}\\\\total_runtime.txt\"\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scores_df.shape)\n",
    "all_scores_df = all_scores_df.drop(columns=[\"run_name\", \"timestamp\", \"index\", \"numerical_encoding\", \"roc_auc\", \"pr_auc\"])\n",
    "all_scores_df[\"buckets\"] = all_scores_df[\"buckets\"].astype(str)\n",
    "print(all_scores_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_results:\n",
    "    model_type = list(results.keys())[0].split('_')[1]\n",
    "    print(model_type)\n",
    "    results_config = [\n",
    "        (f'result_{model_type}_trace_Order', f'labels_{model_type}_trace', Perspective.ORDER, 'trace'),\n",
    "        (f'result_{model_type}_trace_Attribute', f'labels_{model_type}_trace', Perspective.ATTRIBUTE, 'trace'),\n",
    "        (f'result_{model_type}_trace_Arrival Time', f'labels_{model_type}_trace', Perspective.ARRIVAL_TIME, 'trace'),\n",
    "        (f'result_{model_type}_trace_Workload', f'labels_{model_type}_trace', Perspective.WORKLOAD, 'trace'),\n",
    "    ]\n",
    "\n",
    "    nr_buckets = 0\n",
    "    for run in runs:\n",
    "        if run[\"buckets\"] is None:\n",
    "            nr_buckets += 1\n",
    "        else:\n",
    "            nr_buckets += len(run[\"buckets\"])\n",
    "\n",
    "    total_iterations = nr_buckets * len(results_config)\n",
    "    with tqdm(total=total_iterations, desc=\"Generating Plots\") as pbar:\n",
    "        for run in runs:\n",
    "            # print(f\"Generating: {directory}\\t{run_name}\")\n",
    "            for config in results_config:\n",
    "                \n",
    "                # try:\n",
    "                bucket_plot_losses(\n",
    "                    results_name=config[0], \n",
    "                    labels_name=config[1],\n",
    "                    directory=directory,\n",
    "                    experiment_name=experiment_name,\n",
    "                    run_name=run[\"name\"],\n",
    "                    bucket_lengths=run[\"buckets\"],\n",
    "                    results=run[\"results\"],\n",
    "                    perspective=config[2],\n",
    "                    level=config[3],\n",
    "                    zoom=None,\n",
    "                    show_plots=show_plots,\n",
    "                    pbar=pbar)\n",
    "                # except:\n",
    "                #     print(\"Error loading \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the encoding methods combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"perspective\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "\n",
    "averages_methods = (\n",
    "    all_scores_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        # \"roc_auc\": \"mean\", \n",
    "        # \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", #[\"mean\", \"std\"],\n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\", #[\"mean\", \"std\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_methods.columns = ['_'.join(col).strip('_') for col in averages_methods.columns]\n",
    "averages_methods = averages_methods.reset_index()\n",
    "\n",
    "averages_methods[\"categorical_encoding\"] = \"All\"\n",
    "# averages_methods[\"timestamp\"] = \"Average\"\n",
    "\n",
    "result_methods_df = pd.concat([all_scores_df, averages_methods], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_methods.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scores_df.shape)\n",
    "print(averages_methods.shape)\n",
    "print(result_methods_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_single_result_df = result_methods_df[~result_methods_df[\"perspective\"].isin((\"Single\", \"Single_OA\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_single_result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the perspectives combined\n",
    "group_cols = [\n",
    "    \"model\", \"dataset\", \"level\", \"categorical_encoding\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\"\n",
    "]\n",
    "averages_perspective = (\n",
    "    no_single_result_df.groupby(group_cols)\n",
    "    .agg({\n",
    "        # \"roc_auc\": \"mean\", \n",
    "        # \"pr_auc\": \"mean\", \n",
    "        \"f1\": \"mean\", \n",
    "        \"precision\": \"mean\", \n",
    "        \"recall\": \"mean\",\n",
    "        \"run_time\": \"mean\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_perspective.columns = ['_'.join(col).strip('_') for col in averages_perspective.columns]\n",
    "averages_perspective = averages_perspective.reset_index()\n",
    "\n",
    "averages_perspective[\"perspective\"] = \"Average\"\n",
    "\n",
    "\n",
    "result_df = pd.concat([result_methods_df, averages_perspective], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_perspective.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_methods_df.shape)\n",
    "print(averages_perspective.shape)\n",
    "print(result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional columns used in analysis\n",
    "result_df[\"dataset_size\"] = result_df[\"dataset\"].str.split('_').str[3]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"dataset\"].str.split('_').str[4]\n",
    "result_df[\"anomaly_percentage\"] = result_df[\"anomaly_percentage\"].astype(float)\n",
    "result_df[\"batch_size\"] = result_df[\"batch_size\"].astype(str)\n",
    "result_df[\"vector_size\"] = result_df[\"vector_size\"].astype(str)\n",
    "result_df[\"vector_window_size\"] = result_df[\"vector_size\"].astype(str) + '/' + result_df[\"window_size\"].astype(str)\n",
    "# result_df[\"prefix\"] = result_df[\"prefix\"].astype(str)\n",
    "result_df[\"prefix\"] = np.where(\n",
    "    result_df[\"prefix\"].astype(bool), \n",
    "    \"Prefix\", \n",
    "    \"No Prefix\"\n",
    ")\n",
    "result_df[\"buckets\"] = np.where(\n",
    "    result_df[\"buckets\"].astype(str) == \"None\", \n",
    "    \"No Bucketing\", \n",
    "    \"Bucketing\"\n",
    ")\n",
    "result_df[\"prefix_buckets\"] = result_df[\"prefix\"].astype(str) + '/' + result_df[\"buckets\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if score_results:\n",
    "    if experiment_name == \"Experiment_Anomaly_Percentage\":\n",
    "        xlabel_name=\"Anomaly Percentages\"\n",
    "        label_name=\"anomaly_percentage\"\n",
    "        plot_scores(result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(result_df[result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "    elif experiment_name == \"Experiment_Synthetic_Dataset\":\n",
    "        xlabel_name=\"Dataset Sizes\"\n",
    "        label_name=\"dataset_size\"\n",
    "        plot_scores(result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(result_df[result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "    elif experiment_name == \"Experiment_Batch_Size\":\n",
    "        xlabel_name=\"Batch Sizes\"\n",
    "        label_name=\"batch_size\"\n",
    "        plot_scores(result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(result_df[result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "\n",
    "    elif experiment_name == \"Experiment_Prefix_v3\":\n",
    "        xlabel_name=\"Buckets\" # \"Prefix/Buckets\"\n",
    "        label_name=\"buckets\" #\"prefix_buckets\"\n",
    "\n",
    "        sorted_result_df = result_df.sort_values(by=label_name, ascending=True)\n",
    "\n",
    "        plot_scores(sorted_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(sorted_result_df[sorted_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "    \n",
    "    elif experiment_name == \"Experiment_Finetuning_Fixed_Vector_Vector_Sizes\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Fixed Vector\"]\n",
    "\n",
    "        xlabel_name=\"Vector Sizes\"\n",
    "        label_name=\"vector_size\"\n",
    "\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  \n",
    "\n",
    "    elif experiment_name == \"Experiment_Finetuning_T2V_Window_Vector_Sizes\":\n",
    "        atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Average Then Concatinate\"]\n",
    "        c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Trace2Vec Concatinate\"]\n",
    "\n",
    "        xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "        label_name=\"vector_window_size\"\n",
    "        plot_scores(atc_filtered_result_df, directory, experiment_name, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "        plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "        xlabel_name=\"Vector/Window Sizes C\"\n",
    "        plot_scores(c_filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "        plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')      \n",
    "        \n",
    "    elif experiment_name == \"Experiment_Finetuning_W2V_Window_Vector_Sizes\":\n",
    "        atc_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Average Then Concatinate\"]\n",
    "        c_filtered_result_df = result_df[result_df[\"categorical_encoding\"] == \"Word2Vec Concatinate\"]\n",
    "\n",
    "        xlabel_name=\"Vector/Window Sizes ATC\"\n",
    "        label_name=\"vector_window_size\"\n",
    "        plot_scores(atc_filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "        plot_scores(atc_filtered_result_df[atc_filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='ATC')\n",
    "        xlabel_name=\"Vector/Window Sizes C\"\n",
    "        plot_scores(c_filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')\n",
    "        plot_scores(c_filtered_result_df[c_filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name,summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, postfix='C')  \n",
    "\n",
    "    elif experiment_name == \"Experiment_Real_World_All_Models\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"categorical_encoding\"].unique())\n",
    "\n",
    "        xlabel_name=\"Categorical Encoding\"\n",
    "        label_name=\"categorical_encoding\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)  \n",
    "\n",
    "    elif experiment_name == \"Experiment_Synthetic_All_Models\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"categorical_encoding\"].unique())\n",
    "\n",
    "        xlabel_name=\"Categorical Encoding\"\n",
    "        label_name=\"categorical_encoding\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage) \n",
    "\n",
    "    elif experiment_name == \"Experiment_Transformer_Prefix_Store\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"model\"].unique())\n",
    "        print(filtered_result_df[\"perspective\"].unique())\n",
    "\n",
    "        filtered_result_df.sort_values(by=[\"model\",\"level\",\"perspective\"])\n",
    "\n",
    "        xlabel_name=\"Model\"\n",
    "        label_name=\"model\"\n",
    "        legend_title=\"Anomaly Perspectives\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title) \n",
    "\n",
    "    elif experiment_name == \"Experiment_Transformer_Event_Positional_Encoding\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"model\"].unique())\n",
    "        print(filtered_result_df[\"perspective\"].unique())\n",
    "\n",
    "        filtered_result_df.sort_values(by=[\"model\",\"level\",\"perspective\"])\n",
    "\n",
    "        xlabel_name=\"Model\"\n",
    "        label_name=\"model\"\n",
    "        legend_title=\"Anomaly Perspectives\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title)\n",
    "\n",
    "    elif experiment_name == \"Experiment_Transformer_Perspective_Weights_v2\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"model\"].unique())\n",
    "        print(filtered_result_df[\"perspective\"].unique())\n",
    "\n",
    "        filtered_result_df.sort_values(by=[\"model\",\"level\",\"perspective\"])\n",
    "\n",
    "        # All models trained on a single perspective only\n",
    "        only_models_df = filtered_result_df[filtered_result_df[\"model\"].str.contains(\"only\", case=False, na=False)]\n",
    "        # All models trained on multiple perspectives\n",
    "        other_models_df = filtered_result_df[~filtered_result_df[\"model\"].str.contains(\"only\", case=False, na=False)]\n",
    "\n",
    "        xlabel_name=\"Model\"\n",
    "        label_name=\"model\"\n",
    "        legend_title=\"Anomaly Perspectives\"\n",
    "        postfix='single_perspective'\n",
    "        plot_scores(only_models_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)\n",
    "        plot_scores(only_models_df[only_models_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)\n",
    "        postfix='weighted_perspective'\n",
    "        plot_scores(other_models_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)\n",
    "        plot_scores(other_models_df[other_models_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)    \n",
    "        postfix=None\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, postfix=postfix)    \n",
    "    \n",
    "\n",
    "    elif experiment_name == \"Experiment_Transformer_Event_Multi_Task\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"model\"].unique())\n",
    "        print(filtered_result_df[\"perspective\"].unique())\n",
    "\n",
    "        filtered_result_df.sort_values(by=[\"model\",\"level\",\"perspective\"])\n",
    "\n",
    "        xlabel_name=\"Model\"\n",
    "        label_name=\"model\"\n",
    "        legend_title=\"Anomaly Perspectives\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title)\n",
    "        plot_scores(filtered_result_df[filtered_result_df[\"perspective\"] == \"Average\"], directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title)  \n",
    "\n",
    "    elif experiment_name == \"Experiment_Offline_Training\":\n",
    "        filtered_result_df = result_df[result_df[\"categorical_encoding\"] != \"All\"]\n",
    "\n",
    "        print(filtered_result_df[\"model\"].unique())\n",
    "        print(filtered_result_df[\"perspective\"].unique())\n",
    "\n",
    "        filtered_result_df.sort_values(by=[\"model\",\"level\",\"perspective\"])\n",
    "        average_filtered_result_df = filtered_result_df[filtered_result_df[\"perspective\"].isin([\"Average\", \"Single\", \"Single_OA\"])]\n",
    "\n",
    "        run_time = False\n",
    "        xlabel_name=\"Model\"\n",
    "        label_name=\"model\"\n",
    "        legend_title=\"Anomaly Perspectives\"\n",
    "        plot_scores(filtered_result_df, directory, experiment_name, \"all_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, run_time=run_time)\n",
    "        plot_scores(average_filtered_result_df, directory, experiment_name, \"averaged_perspectives\", label_name=label_name, xlabel_name=xlabel_name, summary=score_summary, filter_beginning_percentage=filter_beginning_percentage, legend_title=legend_title, run_time=run_time)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df[\"dataset_short_name\"] = all_scores_df[\"dataset\"].str.split('_').str[3] + \"/\" + all_scores_df[\"dataset\"].str.split('_').str[4]\n",
    "all_scores_df[\"prefix_boolean\"] = np.where(\n",
    "    all_scores_df[\"prefix\"].astype(bool), \n",
    "    \"Prefix\", \n",
    "    \"No Prefix\"\n",
    ")\n",
    "all_scores_df[\"buckets_boolean\"] = np.where(\n",
    "    all_scores_df[\"buckets\"].astype(str) == \"None\", \n",
    "    \"No Bucketing\", \n",
    "    \"Bucketing\"\n",
    ")\n",
    "all_scores_df[\"prefix_buckets\"] = all_scores_df[\"prefix_boolean\"].astype(str) + '/' + all_scores_df[\"buckets_boolean\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grouped_levels(df, group_by_index='dataset_short_name', rank_groupby_index='categorical_encoding'):\n",
    "    grouped_all_levels = (\n",
    "        df.groupby([group_by_index, rank_groupby_index, 'level'])[['f1']]\n",
    "        .agg(['mean', 'std'])\n",
    "        .reset_index()\n",
    "    )\n",
    "    # print(grouped_all_levels.shape)\n",
    "\n",
    "    grouped_combined_levels = (\n",
    "        df.groupby([group_by_index, rank_groupby_index])[['f1']]\n",
    "        .agg(['mean', 'std'])\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped_combined_levels['level'] = 'combined'\n",
    "    # print(grouped_combined_levels.shape)\n",
    "    \n",
    "    grouped_levels = pd.concat([grouped_all_levels, grouped_combined_levels], axis=0, ignore_index=True)\n",
    "    grouped_levels.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in grouped_levels.columns]\n",
    "    # print(grouped_levels.shape)\n",
    "\n",
    "    grouped_levels['f1_rank'] = grouped_levels.groupby([group_by_index, 'level'])['f1_mean'].rank(ascending=False)\n",
    "    # print(grouped_levels.shape)\n",
    "\n",
    "    return grouped_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'Transformer':\n",
    "    grouped_levels = calculate_grouped_levels(all_scores_df, group_by_index='dataset_short_name', rank_groupby_index='model')\n",
    "else:\n",
    "    grouped_levels = calculate_grouped_levels(all_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"results\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "grouped_levels.to_csv(f'{plot_path}\\grouped_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcualte_rank_stats(df, group_by_index=['categorical_encoding', 'level']):\n",
    "    rank_stats_df = (\n",
    "        df.groupby(group_by_index)\n",
    "        .agg(\n",
    "            rank_mean=('f1_rank', 'mean'),\n",
    "            rank_std=('f1_rank', 'std'),\n",
    "            f1_mean=('f1_mean', 'mean'),\n",
    "            f1_std=('f1_mean', 'std')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    rank_stats_df['rank_std'] = rank_stats_df['rank_std'].fillna(0)\n",
    "    rank_stats_df['f1_std'] = rank_stats_df['f1_std'].fillna(0)\n",
    "\n",
    "    rank_stats_df['rank_mean'] = rank_stats_df['rank_mean'].round(2)\n",
    "    rank_stats_df['rank_std'] = rank_stats_df['rank_std'].round(2)\n",
    "    rank_stats_df['f1_mean'] = rank_stats_df['f1_mean'].round(2)\n",
    "    rank_stats_df['f1_std'] = rank_stats_df['f1_std'].round(2)\n",
    "\n",
    "    if len(group_by_index) == 2:\n",
    "        rank_stats_df = rank_stats_df.sort_values(by=['level', 'rank_mean'], ascending=[True, True])\n",
    "    else:\n",
    "        rank_stats_df = rank_stats_df.sort_values(by=['rank_mean'], ascending=[True])\n",
    "        \n",
    "    return rank_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'Transformer':\n",
    "    rank_stats_df = calcualte_rank_stats(grouped_levels, group_by_index=['model', 'level'])\n",
    "else:\n",
    "    rank_stats_df = calcualte_rank_stats(grouped_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"results\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "rank_stats_df.to_csv(f'{plot_path}\\summarised_rank_stats.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_order = ['trace', 'event', 'attribute', 'combined']\n",
    "grouped_levels['level'] = pd.Categorical(grouped_levels['level'], categories=level_order, ordered=True)\n",
    "grouped_levels = grouped_levels.sort_values('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if rank_encoders:\n",
    "    sns.set_theme(style=\"white\")\n",
    "    g = sns.FacetGrid(grouped_levels, col=\"level\", sharey=True, sharex=False, height=5, aspect=1.2, col_wrap=2)\n",
    "\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x=\"dataset_short_name\",\n",
    "        y=\"f1_rank\",\n",
    "        hue=\"categorical_encoding\",\n",
    "        marker=\"o\"\n",
    "    )\n",
    "\n",
    "    g.set_axis_labels(\"Dataset\", \"Rank (1 = Best)\")\n",
    "    g.set_titles(\"Granularity: {col_name}\")\n",
    "    # plt.subplots_adjust(top=0.85)\n",
    "    # g.figure.suptitle(f\"{experiment_name}: Ranking of Categorical Encodings methods per Dataset and Level\", fontsize=16)\n",
    "\n",
    "    # for ax in g.axes.flatten():\n",
    "    #     ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    #     ax.grid(False)\n",
    "\n",
    "    for ax in g.axes.flatten():\n",
    "        # Get the current tick positions and labels\n",
    "        tick_positions = ax.get_xticks()\n",
    "        tick_labels = ax.get_xticklabels()\n",
    "\n",
    "        # Set the ticks and labels explicitly with rotation\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "\n",
    "        ax.grid(False)  # Turn off gridlines\n",
    "        \n",
    "    # g.add_legend(title=\"Categorical Encoding\", loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    g.add_legend(\n",
    "        title=\"Categorical Encoding\", \n",
    "        loc='lower center', \n",
    "        bbox_to_anchor=(0.5, -0.05), \n",
    "        ncol=len(grouped_levels[\"categorical_encoding\"].unique()),\n",
    "        title_fontsize=14,\n",
    "        prop={'size': 12})\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = f\"results\\{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    plt.savefig(f\"{plot_path}\\encoding_ranking_per_dataset.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_rankings(data_df, title, rank_column='rank_mean', error_column='rank_std', sharey=True):\n",
    "    levels = ['combined', 'trace', 'event', 'attribute']\n",
    "    fig, axes = plt.subplots(1, len(levels), figsize=(15, 4), sharey=sharey)\n",
    "\n",
    "    for i, level in enumerate(levels):\n",
    "        ax = axes[i]\n",
    "        level_data = data_df[data_df['level'] == level]\n",
    "        \n",
    "        level_data_sorted = level_data.sort_values(rank_column)\n",
    "        ordered_categories = level_data_sorted['categorical_encoding']\n",
    "        y_values = level_data_sorted[rank_column]\n",
    "        errors = level_data_sorted[error_column]\n",
    "\n",
    "        ax.bar(\n",
    "            ordered_categories, \n",
    "            y_values, \n",
    "            yerr=errors,\n",
    "            color='skyblue', \n",
    "            capsize=5,\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Level: {level}\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Average Rank (Lower = Better)\")\n",
    "        ax.tick_params(axis=\"x\", rotation=90)\n",
    "        ax.grid(False)\n",
    "        ax.grid(axis='y', linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    fig.suptitle(f\"{title}: Average {rank_column} with Uncertainty Margins per Level\", fontsize=16)\n",
    "\n",
    "    plot_path = f\"results/{directory}\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    plt.savefig(f\"{plot_path}/encoding_rankings_summary_{rank_column}.png\", format='png', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank_encoders:\n",
    "    plot_rankings(rank_stats_df, title='Rank', rank_column='rank_mean', error_column='rank_std', sharey=True)\n",
    "    plot_rankings(rank_stats_df, title='F1', rank_column='f1_mean', error_column='f1_std', sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Difference Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Orange\n",
    "\n",
    "# Dictionary mapping old values to new values\n",
    "cd_rename_dict = {\n",
    "    \"One Hot\": \"One-Hot\",\n",
    "    \"Word2Vec Concatinate\": \"Word2Vec C\",\n",
    "    \"Word2Vec Average Then Concatinate\": \"Word2Vec ATC\",\n",
    "    \"Trace2Vec Concatinate\": \"Trace2Vec C\",\n",
    "    \"Trace2Vec Average Then Concatinate\": \"Trace2Vec ATC\",\n",
    "    \"Fixed Vector\": \"Fixed-Vector\"\n",
    "}\n",
    "\n",
    "def generate_cd_plot(rank_stats, level, nr_datasets, perspective, plot_path, group_by_index, sub_filter_encoding):\n",
    "    rank_stats_lvl = rank_stats # rank_stats[rank_stats[\"level\"] == level]\n",
    "    names = rank_stats_lvl[group_by_index].to_list()\n",
    "    avranks = rank_stats_lvl['rank_mean'].to_list()\n",
    "\n",
    "    # Apply the rename using the dictionary\n",
    "    names = [cd_rename_dict.get(name, name) for name in names]\n",
    "\n",
    "    print(names)\n",
    "\n",
    "    if sub_filter_encoding:\n",
    "        filename = f\"{plot_path}/CD_{sub_filter_encoding}_{perspective}_{level}.png\"\n",
    "    else:\n",
    "        filename = f\"{plot_path}/CD_{perspective}_{level}.png\"\n",
    "\n",
    "    # https://github.com/biolab/orange3/blob/e2282bbe2cf3d30ba41220ea2df0b79201ca430c/Orange/evaluation/scoring.py\n",
    "    # Choosing Nemenyi as no method is seen as the baseline to which all others are compared\n",
    "    cd = Orange.evaluation.scoring.compute_CD(avranks=avranks, n=nr_datasets, alpha=\"0.05\", test='nemenyi')\n",
    "    # print(cd)\n",
    "    Orange.evaluation.scoring.graph_ranks(avranks=avranks, names=names, cd=cd, width=6, textspace=1.4, reverse=True, filename=filename) #textspace=3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def calculate_nr_repeats(df, group_by_index='dataset_short_name', rank_groupby_index='categorical_encoding'):\n",
    "    unique_combinations = df[[group_by_index, rank_groupby_index, 'level','perspective']].drop_duplicates()\n",
    "    return floor(df.shape[0]/len(unique_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_filter_encodings = [None]\n",
    "if experiment_name == \"Experiment_Anomaly_Percentage\":\n",
    "    level_groupby_index =  'categorical_encoding'\n",
    "    rank_groupby_index = 'dataset_short_name'\n",
    "elif experiment_name == \"Experiment_Finetuning_T2V_Window_Vector_Sizes\" or experiment_name == \"Experiment_Finetuning_W2V_Window_Vector_Sizes\" or experiment_name == \"Experiment_Finetuning_Fixed_Vector_Vector_Sizes\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'categorical_encoding'\n",
    "    sub_filter_encodings = all_scores_df[\"categorical_encoding\"].unique()\n",
    "    all_scores_df[\"categorical_encoding\"] = all_scores_df[\"categorical_encoding\"] + \"[\" + all_scores_df[\"vector_size\"].astype(str) + \",\" + all_scores_df[\"window_size\"].astype(str) + \"]\"\n",
    "elif experiment_name == \"Experiment_Finetuning_Fixed_Vector_Vector_Sizes\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'categorical_encoding'\n",
    "    sub_filter_encodings = all_scores_df[\"categorical_encoding\"].unique()\n",
    "    all_scores_df[\"categorical_encoding\"] = all_scores_df[\"categorical_encoding\"] + \"[\" + all_scores_df[\"vector_size\"].astype(str) + \"]\"\n",
    "elif experiment_name == \"Experiment_Prefix_v3\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'buckets_boolean' # 'prefix_buckets'\n",
    "elif experiment_name == \"Experiment_Transformer_Prefix_Store\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'model' \n",
    "elif experiment_name == \"Experiment_Transformer_Event_Positional_Encoding\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'model'\n",
    "elif experiment_name == \"Experiment_Transformer_Perspective_Weights\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'model'    \n",
    "elif experiment_name == \"Experiment_Transformer_Perspective_Multi_Task\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'model'\n",
    "elif experiment_name == \"Experiment_Offline_Training\":\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'model'\n",
    "else: # Default to rank by encoder unless otherwise specified\n",
    "    level_groupby_index = 'dataset_short_name'\n",
    "    rank_groupby_index = 'categorical_encoding'\n",
    "\n",
    "# print(sub_filter_encodings)\n",
    "# print(all_scores_df[\"categorical_encoding\"].unique())\n",
    "\n",
    "all_rank_stats = []\n",
    "if rank_cd_encoders:\n",
    "    plot_path = f\"results/{directory}/CD_ranking\"\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "    # Effective number of datasets because the critical difference depends on the total number of test results, not necessarily their source.\n",
    "    nr_datasets = len(all_scores_df[level_groupby_index].unique()) * calculate_nr_repeats(all_scores_df)\n",
    "    print(nr_datasets)\n",
    "    perspectives = all_scores_df['perspective'].unique()\n",
    "    perspectives = np.append(perspectives, 'Average')\n",
    "\n",
    "    levels = [\"combined\", \"trace\", \"event\", \"attribute\"]\n",
    "    for sub_filter_encoding in sub_filter_encodings:\n",
    "        if sub_filter_encoding is not None:\n",
    "            all_scores_df_filtered = all_scores_df[all_scores_df['categorical_encoding'].str.contains(sub_filter_encoding)]\n",
    "           # print(scoring_df.shape)\n",
    "        else:\n",
    "            all_scores_df_filtered = all_scores_df\n",
    "\n",
    "        for perspective in perspectives:\n",
    "            for level in levels:\n",
    "                # print(f'filtering {perspective}')\n",
    "                # print(perspective)\n",
    "                if perspective != 'Average':\n",
    "                    scoring_df = all_scores_df_filtered[all_scores_df_filtered['perspective'] == perspective]\n",
    "                else:\n",
    "                    # print('Not filtering')\n",
    "                    # print(f'shape before perspective filtering {all_scores_df_filtered.shape}')\n",
    "                    scoring_df = all_scores_df_filtered\n",
    "                    # print(f'shape after perspective filtering {scoring_df.shape}')\n",
    "                \n",
    "                # print(grouped_levels.shape)\n",
    "                grouped_levels = calculate_grouped_levels(scoring_df,group_by_index=level_groupby_index, rank_groupby_index=rank_groupby_index)\n",
    "                grouped_levels = grouped_levels[grouped_levels['level'] == level]\n",
    "                # print(grouped_levels.columns)\n",
    "            \n",
    "                rank_stats_df = calcualte_rank_stats(grouped_levels, group_by_index=[rank_groupby_index])\n",
    "                # print(rank_stats_df)\n",
    "\n",
    "                generate_cd_plot(rank_stats_df, level=level, nr_datasets=nr_datasets, perspective=perspective, plot_path=plot_path, group_by_index=rank_groupby_index, sub_filter_encoding=sub_filter_encoding)\n",
    "\n",
    "                rank_stats_concat = rank_stats_df.copy()\n",
    "                rank_stats_concat['perspective'] = perspective\n",
    "                rank_stats_concat['level'] = level\n",
    "                all_rank_stats.append(rank_stats_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table = all_scores_df.drop(columns=[\"precision\", \"recall\", \"batch_size\", \"vector_size\", \"window_size\", \"prefix\", \"buckets\", \"prefix_boolean\", \"buckets_boolean\", \"prefix_buckets\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table = scores_table.drop(columns=[\"model\", \"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table = scores_table[[\"dataset_short_name\", \"level\", \"categorical_encoding\", \"perspective\", \"f1\", \"run_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table_no_single_result = scores_table[~scores_table[\"perspective\"].isin((\"Single\", \"Single_OA\"))]\n",
    "print(scores_table_no_single_result[\"perspective\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the perspectives combined\n",
    "group_cols = [\n",
    "    \"dataset_short_name\", \"level\", \"categorical_encoding\"\n",
    "]\n",
    "averages_perspective = (\n",
    "    scores_table_no_single_result.groupby(group_cols)\n",
    "    .agg({\n",
    "        \"f1\": \"mean\",\n",
    "        # \"f1\": \"std\",\n",
    "        \"run_time\": \"mean\",\n",
    "        # \"run_time\": \"std\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_perspective.columns = ['_'.join(col).strip('_') for col in averages_perspective.columns]\n",
    "averages_perspective = averages_perspective.reset_index()\n",
    "\n",
    "averages_perspective[\"perspective\"] = \"Average\"\n",
    "\n",
    "\n",
    "scores_table = pd.concat([scores_table, averages_perspective], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional rows for the perspectives combined\n",
    "group_cols = [\n",
    "    \"dataset_short_name\", \"perspective\", \"categorical_encoding\"\n",
    "]\n",
    "combined_levels = (\n",
    "    scores_table.copy().groupby(group_cols)\n",
    "    .agg({\n",
    "        \"f1\": \"mean\",\n",
    "        # \"f1\": \"std\",\n",
    "        \"run_time\": \"mean\",\n",
    "        # \"run_time\": \"std\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# averages_perspective.columns = ['_'.join(col).strip('_') for col in averages_perspective.columns]\n",
    "combined_levels = combined_levels.reset_index()\n",
    "\n",
    "combined_levels[\"level\"] = \"combined\"\n",
    "\n",
    "\n",
    "scores_table = pd.concat([scores_table, combined_levels], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_perspective.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_levels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table[\"temp_count\"] = 1\n",
    "scores_table_mean = scores_table.groupby(\n",
    "    [\"level\", \"categorical_encoding\", \"perspective\"]\n",
    "    ).agg(f1_mean=(\"f1\", \"mean\"), f1_std=(\"f1\", \"std\"), run_time_mean=(\"run_time\", \"mean\"), run_time_std=(\"run_time\", \"std\"), temp_count=(\"temp_count\", \"sum\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table_mean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_stats_df = pd.concat(all_rank_stats, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_stats_df.drop(columns=['f1_mean', 'f1_std', 'rank_std'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_stats_df = all_rank_stats_df[[\"level\", \"categorical_encoding\", \"perspective\", \"rank_mean\"]]\n",
    "all_rank_stats_df = all_rank_stats_df.rename(columns={\"rank_mean\": \"rank\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_table['perspective'].unique())\n",
    "print(all_rank_stats_df['perspective'].unique())\n",
    "print(scores_table['level'].unique())\n",
    "print(all_rank_stats_df['level'].unique())\n",
    "print(scores_table_mean.shape)\n",
    "print(all_rank_stats_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_stats_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table_mean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the multi-columns\n",
    "# scores_table_mean.columns = ['_'.join(col).strip('_') for col in scores_table_mean.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table_mean_rank = pd.merge(scores_table_mean, all_rank_stats_df, on=[\"level\", \"categorical_encoding\", \"perspective\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_table_mean_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_pivot = scores_table_mean_rank.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the pooled standard deviation\n",
    "def pooled_std(group):\n",
    "    n = group[\"temp_count\"]\n",
    "    stds = group[\"run_time_std\"]\n",
    "    return np.sqrt(((n - 1) * (stds ** 2)).sum() / (n.sum() - 1))\n",
    "\n",
    "# Apply pooled standard deviation calculation\n",
    "runtime_stds_pooled = score_table_pivot.groupby([\"level\", \"categorical_encoding\"]).apply(pooled_std).reset_index(name=\"run_time_std\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_stds_pooled.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate run_time_mean and run_time_std across all perspectives\n",
    "runtime_mean_pooled = score_table_pivot.groupby([\"level\", \"categorical_encoding\"])[[\"run_time_mean\"]].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot = score_table_pivot.pivot(index=[\"level\", \"categorical_encoding\"], columns=\"perspective\", values=[\"f1_mean\", \"f1_std\", \"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot.columns = [f\"{metric}_{persp}\" for metric, persp in score_table_perspective_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot_final = pd.merge(score_table_perspective_pivot, runtime_mean_pooled, on=[\"level\", \"categorical_encoding\"], how=\"left\")\n",
    "score_table_perspective_pivot_final = pd.merge(score_table_perspective_pivot_final, runtime_stds_pooled, on=[\"level\", \"categorical_encoding\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct column order\n",
    "ordered_columns = [\n",
    "    \"level\", \"categorical_encoding\",\n",
    "    # Average Perspective\n",
    "    \"rank_Average\", \"f1_mean_Average\", \"f1_std_Average\",\n",
    "    # Single Perspective\n",
    "    \"rank_Single\", \"f1_mean_Single\", \"f1_std_Single\",\n",
    "    # Single_OA Perspective\n",
    "    \"rank_Single_OA\", \"f1_mean_Single_OA\", \"f1_std_Single_OA\",\n",
    "    # Order Perspective\n",
    "    \"rank_Order\", \"f1_mean_Order\", \"f1_std_Order\",\n",
    "    # Attribute Perspective\n",
    "    \"rank_Attribute\", \"f1_mean_Attribute\", \"f1_std_Attribute\",\n",
    "    # Arrival Time Perspective\n",
    "    \"rank_Arrival Time\", \"f1_mean_Arrival Time\", \"f1_std_Arrival Time\",\n",
    "    # Workload Perspective\n",
    "    \"rank_Workload\", \"f1_mean_Workload\", \"f1_std_Workload\",\n",
    "    # Runtime Statistics\n",
    "    \"run_time_mean\", \"run_time_std\"\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame\n",
    "score_table_perspective_pivot_final = score_table_perspective_pivot_final[ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom order for 'level'\n",
    "level_order = [\"trace\", \"event\", \"attribute\", \"combined\"]\n",
    "\n",
    "# Custom order for 'categorical_encoding'\n",
    "categorical_encoding_order = [\n",
    "    \"One Hot\", \"Word2Vec Concatinate\", \"Word2Vec Average Then Concatinate\", \n",
    "    \"Trace2Vec Concatinate\", \"Trace2Vec Average Then Concatinate\", \"Fixed Vector\"\n",
    "]\n",
    "\n",
    "# Apply the custom order to the 'level' column\n",
    "score_table_perspective_pivot_final['level'] = pd.Categorical(score_table_perspective_pivot_final['level'], categories=level_order, ordered=True)\n",
    "\n",
    "# Apply the custom order to the 'categorical_encoding' column\n",
    "score_table_perspective_pivot_final['categorical_encoding'] = pd.Categorical(score_table_perspective_pivot_final['categorical_encoding'], categories=categorical_encoding_order, ordered=True)\n",
    "\n",
    "# Now sort the DataFrame based on the custom order\n",
    "score_table_perspective_pivot_final = score_table_perspective_pivot_final.sort_values(by=[\"level\", \"categorical_encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old values to new values\n",
    "rename_dict = {\n",
    "    \"One Hot\": \"One-Hot\",\n",
    "    \"Word2Vec Concatinate\": \"Word2Vec C\",\n",
    "    \"Word2Vec Average Then Concatinate\": \"Word2Vec ATC\",\n",
    "    \"Trace2Vec Concatinate\": \"Trace2Vec C\",\n",
    "    \"Trace2Vec Average Then Concatinate\": \"Trace2Vec ATC\",\n",
    "    \"Fixed Vector\": \"Fixed-Vector\"\n",
    "}\n",
    "\n",
    "# Apply the rename using the dictionary\n",
    "score_table_perspective_pivot_final['categorical_encoding'] = score_table_perspective_pivot_final['categorical_encoding'].replace(rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot_final = score_table_perspective_pivot_final.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table_perspective_pivot_final_no_single = score_table_perspective_pivot_final.drop(columns=[\"rank_Single\", \"f1_mean_Single\", \"f1_std_Single\", \"rank_Single_OA\", \"f1_mean_Single_OA\", \"f1_std_Single_OA\"])\n",
    "score_table_perspective_pivot_final_no_single_no_rank = score_table_perspective_pivot_final_no_single.drop(columns=[\"rank_Average\", \"rank_Order\", \"rank_Attribute\", \"rank_Arrival Time\", \"rank_Workload\"])\n",
    "score_table_perspective_pivot_final_no_single_only_rank = score_table_perspective_pivot_final_no_single[[\"level\", \"categorical_encoding\", \"rank_Average\", \"rank_Order\", \"rank_Attribute\", \"rank_Arrival Time\", \"rank_Workload\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = f\"results\\{directory}\"\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "score_table_perspective_pivot_final.to_csv(f'{plot_path}\\paper_table.csv', index=False, sep='&')\n",
    "score_table_perspective_pivot_final_no_single.to_csv(f'{plot_path}\\paper_table_no_single.csv', index=False, sep='&')\n",
    "score_table_perspective_pivot_final_no_single_no_rank.to_csv(f'{plot_path}\\paper_table_no_single_no_rank.csv', index=False, sep='&')\n",
    "score_table_perspective_pivot_final_no_single_only_rank.to_csv(f'{plot_path}\\paper_table_no_single_only_rank.csv', index=False, sep='&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
