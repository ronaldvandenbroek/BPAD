{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that local modules are reloaded when edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that plots are displayed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/training-the-transformer-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ronal\\miniconda3\\envs\\rcvdb-thesis-bpad\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncodingNumerical.MIN_MAX_SCALING EncodingNumerical.MIN_MAX_SCALING\n",
      "0.0 81.0\n",
      "0.0 1.0\n",
      "EncodingNumerical.MIN_MAX_SCALING EncodingNumerical.MIN_MAX_SCALING\n",
      "0.0 159.0\n",
      "0.0 1.0\n",
      "EncodingNumerical.MIN_MAX_SCALING EncodingNumerical.MIN_MAX_SCALING\n",
      "0.0 27.0\n",
      "0.0 1.0\n",
      "EncodingNumerical.MIN_MAX_SCALING EncodingNumerical.MIN_MAX_SCALING\n",
      "0.0 54.0\n",
      "0.0 1.0\n",
      "EncodingNumerical.MIN_MAX_SCALING EncodingNumerical.MIN_MAX_SCALING\n",
      "0.0 12.0\n",
      "0.0 1.0\n",
      "Feature Columns: dict_keys(['name', 'arrival_time', 'company', 'country', 'department', 'global_workload_D', 'global_workload_h', 'local_workload_D', 'local_workload_h', 'user'])\n",
      "Feature Shape: (35483, 13)\n",
      "Case Length: [ 2  2  3 ...  8 10 10]\n",
      "Attribute Types: [<AttributeType.CATEGORICAL: 0>, <AttributeType.NUMERICAL: 1>, <AttributeType.CATEGORICAL: 0>, <AttributeType.CATEGORICAL: 0>, <AttributeType.CATEGORICAL: 0>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.CATEGORICAL: 0>]\n",
      "Encoders: {'name': AttributeDictionary (size=61, indexes=[3,64], reserved=74), 'company': AttributeDictionary (size=7, indexes=[80,87], reserved=-71), 'country': AttributeDictionary (size=17, indexes=[89,106], reserved=-67), 'department': AttributeDictionary (size=17, indexes=[111,128], reserved=-89), 'user': AttributeDictionary (size=102, indexes=[133,235], reserved=-5)}\n",
      "Attribute Labels shape: (35483, 13, 10, 4)\n",
      "Event Labels shape: (35483, 13, 4)\n",
      "Case Labels shape: (35483, 4)\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import Dataset\n",
    "from utils.enums import EncodingCategorical, EncodingNumerical\n",
    "\n",
    "dataset_name='medium_debug_v2-0.15-4_1.json.gz'\n",
    "prefix=True\n",
    "pretrain_percentage=0.0\n",
    "w2v_vector_size=20\n",
    "w2v_window_size=4\n",
    "categorical_encoding=EncodingCategorical.WORD_2_VEC\n",
    "numerical_encoding=EncodingNumerical.MIN_MAX_SCALING\n",
    "fs_save=None\n",
    "\n",
    "dataset = Dataset(dataset_name, \n",
    "                      beta=0.005, \n",
    "                      prefix=prefix,\n",
    "                      pretrain_percentage=pretrain_percentage,\n",
    "                      w2v_vector_size=w2v_vector_size,\n",
    "                      w2v_window_size=w2v_window_size, \n",
    "                      categorical_encoding=categorical_encoding,\n",
    "                      numerical_encoding=numerical_encoding,\n",
    "                      fs_save=fs_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding.w2v import ProcessWord2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 4, 5, 5, 5, 5, 3, 2, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.case_lens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.491164783135586"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.case_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 82.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84., 85.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84., 85., 85.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83., 86.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83., 86.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83., 86.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83., 86.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84., 82.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 82., 83.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1., 84., 85.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=200)\n",
    "dataset.features[2][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.01886792, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.01886792, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.01886792, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.01886792, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01886792, 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.22012579, 0.22012579, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=300)\n",
    "dataset.features[5][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_encoder = ProcessWord2Vec(\n",
    "            encoders=dataset.encoders,\n",
    "            pretrain_percentage=dataset.pretrain_percentage,\n",
    "            attribute_types=dataset.attribute_types,\n",
    "            event_attribute_keys=dataset.event_log.event_attribute_keys,\n",
    "            features=dataset._features,\n",
    "            event_log=dataset.event_log,\n",
    "            vector_size=dataset.w2v_vector_size,\n",
    "            window=dataset.w2v_window_size,\n",
    "            fs_save=dataset.fs_save) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_features, numeric_features, numeric_feature_names, w2v_feature_names = w2v_encoder.encode_features(average=False, match_numerical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 35483, 13, 20)\n",
      "(35483, 13, 5, 20)\n"
     ]
    }
   ],
   "source": [
    "# (num_attribute, num_cases, num_events, vector_size) \n",
    "print(w2v_features.shape)\n",
    "transposed_w2v_features = np.transpose(w2v_features, (1, 2, 0, 3))\n",
    "print(transposed_w2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 35483, 13, 20)\n",
      "(35483, 13, 5, 20)\n"
     ]
    }
   ],
   "source": [
    "print(numeric_features.shape)\n",
    "transposed_numeric_features = np.transpose(numeric_features, (1, 2, 0, 3))\n",
    "print(transposed_numeric_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.case_lens[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.07749242, 0.1545188 , 0.23061587, 0.30532598, 0.37819985, 0.4487992 , 0.5166994 , 0.58149207, 0.64278764, 0.70021737, 0.99699295, 0.98798984, 0.9730449 , 0.95224786, 0.92572397, 0.89363265, 0.8561669 , 0.8135521 , 0.76604444, 0.7139297 ],\n",
       "       [0.11609291, 0.23061587, 0.34202015, 0.4487992 , 0.549509  , 0.64278764, 0.72737366, 0.8021232 , 0.8660254 , 0.9182161 , 0.99323833, 0.9730449 , 0.9396926 , 0.89363265, 0.8354878 , 0.76604444, 0.6862416 , 0.5971586 , 0.5       , 0.39607978],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_numeric_features[12,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01569275,  0.03015446, -0.03076373, -0.0099005 , -0.02991541, -0.0049784 , -0.01010493,  0.04242973,  0.00039001, -0.04287663, -0.02714549, -0.03437993,  0.01346191,  0.04728324, -0.02907998,  0.04132513,  0.04266026, -0.0353132 , -0.04441606,  0.04734592],\n",
       "       [ 0.02424864, -0.03082013,  0.01259593,  0.0036972 , -0.01696077, -0.00489612,  0.04989563,  0.04572944, -0.02230915,  0.04541513, -0.02820882,  0.02965461, -0.01548609,  0.01715876,  0.01508613,  0.03450231, -0.01186942,  0.04387518,  0.03794714, -0.04773823],\n",
       "       [-0.04004105, -0.03818948,  0.01461629, -0.01397361, -0.0346476 , -0.04064132,  0.0415459 ,  0.00995244, -0.04664009, -0.02396358,  0.01568369, -0.02356603,  0.02640421, -0.02116721,  0.01320898, -0.04022844,  0.03104943,  0.02409444,  0.00393596,  0.01506724],\n",
       "       [ 0.04776092, -0.03678212, -0.03635194, -0.01132694, -0.0038928 , -0.01608052, -0.00296293,  0.03744411, -0.00348759, -0.0081247 ,  0.013722  , -0.0417955 ,  0.03927902,  0.04268052, -0.04792044,  0.01223133,  0.04952485, -0.03832902, -0.03483459, -0.03868259],\n",
       "       [ 0.00890996, -0.0341445 , -0.04862406,  0.04520292,  0.03099027, -0.03456464,  0.01701741,  0.00103032,  0.02376873, -0.03559972,  0.02013477,  0.02173717,  0.04978685, -0.0223687 , -0.00694632, -0.03658661, -0.04848915, -0.04540129, -0.00511377, -0.03251645],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_w2v_features[12,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged features shape: (35483, 13, 10, 20)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the input data\n",
    "num_traces, num_events, num_numeric_features, vector_size = transposed_numeric_features.shape\n",
    "_, _, num_w2v_features, _ = transposed_w2v_features.shape\n",
    "\n",
    "# Initialize the merged array\n",
    "merged_features = np.zeros((num_traces, num_events, num_numeric_features + num_w2v_features, vector_size))\n",
    "\n",
    "# Keep track of the current indices for numeric and w2v features\n",
    "numeric_index, w2v_index = 0, 0\n",
    "\n",
    "# Iterate over dataset.attribute_keys to place each feature in the correct order\n",
    "for i, key in enumerate(dataset.attribute_keys):\n",
    "    # print(key)\n",
    "    if key in numeric_feature_names:\n",
    "        # print(\"Numeric feature shape:\", transposed_numeric_features[:, :, numeric_index, :].shape)\n",
    "        # print(\"Target shape:\", merged_features[:, :, i, :].shape)\n",
    "        # print(numeric_index)\n",
    "        # Place numeric feature in the merged array\n",
    "        merged_features[:, :, i, :] = transposed_numeric_features[:, :, numeric_index, :]\n",
    "        numeric_index += 1\n",
    "    elif key in w2v_feature_names:\n",
    "        # print(\"W2V feature shape:\", transposed_w2v_features[:, :, w2v_index, :].shape)\n",
    "        # print(\"Target shape:\", merged_features[:, :, i, :].shape)\n",
    "        # print(w2v_index)\n",
    "        # Place w2v feature in the merged array\n",
    "        merged_features[:, :, i, :] = transposed_w2v_features[:, :, w2v_index, :]\n",
    "        w2v_index += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected attribute key '{key}' not found in either feature list.\")\n",
    "\n",
    "# The merged array now has features interlaced according to dataset.attribute_keys order\n",
    "print(\"Merged features shape:\", merged_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 5, 5, 5, 5, 3, 2, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.case_lens[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00890996, -0.0341445 , -0.04862406,  0.04520292,  0.03099027, -0.03456464,  0.01701741,  0.00103032,  0.02376873, -0.03559972,  0.02013477,  0.02173717,  0.04978685, -0.0223687 , -0.00694632, -0.03658661, -0.04848915, -0.04540129, -0.00511377, -0.03251645],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.03949034, -0.03494752, -0.04577933, -0.00177876, -0.0154992 ,  0.03947159,  0.02969287, -0.00772831,  0.00755482,  0.0089502 ,  0.03908785, -0.04755094, -0.00102766,  0.01734598, -0.00469486,  0.04190886,  0.04505392,  0.03268253, -0.00355811,  0.03855202],\n",
       "       [-0.04801775,  0.02503647, -0.04379793, -0.02195913, -0.0001755 , -0.00148091, -0.0383062 ,  0.04807372,  0.02491029,  0.04616572, -0.04078959,  0.02247899, -0.02068538,  0.00412268,  0.0424931 , -0.02231088,  0.0225875 , -0.0339348 , -0.01774244,  0.04699254],\n",
       "       [-0.04801775,  0.02503647, -0.04379793, -0.02195913, -0.0001755 , -0.00148091, -0.0383062 ,  0.04807372,  0.02491029,  0.04616572, -0.04078959,  0.02247899, -0.02068538,  0.00412268,  0.0424931 , -0.02231088,  0.0225875 , -0.0339348 , -0.01774244,  0.04699254],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.0408406 , -0.02221516,  0.04492717,  0.04126833, -0.02217611,  0.00151553,  0.02137246, -0.0196316 , -0.02779983, -0.03256161, -0.00335369, -0.00147961,  0.02231542, -0.01237027, -0.00086305,  0.01230938,  0.024338  , -0.00015404, -0.03169705, -0.04630404]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_features[12][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_event = np.zeros((num_numeric_features + num_w2v_features, vector_size))\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "for case, length in zip(merged_features, dataset.case_lens):\n",
    "    last_event_index = length - 1\n",
    "    trainY.append(case[length - 1].copy())\n",
    "\n",
    "    # Remove the target event from the training data\n",
    "    case[length - 1] = zero_event\n",
    "    trainX.append(case)\n",
    "\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 13, 10, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 10, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.42486410e-02, -3.08201313e-02,  1.25959273e-02,  3.69720464e-03, -1.69607699e-02, -4.89611644e-03,  4.98956256e-02,  4.57294360e-02, -2.23091487e-02,  4.54151332e-02, -2.82088164e-02,  2.96546109e-02, -1.54860914e-02,  1.71587579e-02,  1.50861321e-02,  3.45023051e-02, -1.18694184e-02,\n",
       "         4.38751802e-02,  3.79471406e-02, -4.77382317e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-4.28278465e-03,  1.41328154e-02,  2.70071439e-02,  3.52632813e-02, -2.85156071e-02,  9.29409824e-03,  3.04443184e-02, -2.39902548e-02, -1.55363027e-02,  3.39881480e-02,  8.15737806e-03,  9.49585461e-04,  1.73681863e-02,  1.08888745e-03,  4.80941311e-02,  2.53030173e-02, -4.45869491e-02,\n",
       "        -3.52078006e-02,  4.50727949e-03,  3.19626704e-02],\n",
       "       [-4.30984385e-02,  1.83286909e-02,  2.59494185e-02,  2.87096910e-02,  3.73345912e-02, -3.08383759e-02,  5.52806864e-03,  3.02364118e-02, -1.42002525e-02, -3.08676120e-02, -2.05111504e-03, -4.18447442e-02, -2.80000623e-02,  3.55226919e-02,  1.67626981e-02,  3.61283496e-02,  3.40012386e-02,\n",
       "         3.76537070e-02, -1.89457722e-02, -2.80902977e-03],\n",
       "       [ 3.94903421e-02, -3.49475220e-02, -4.57793251e-02, -1.77876349e-03, -1.54992044e-02,  3.94715853e-02,  2.96928696e-02, -7.72831449e-03,  7.55481701e-03,  8.95020366e-03,  3.90878543e-02, -4.75509353e-02, -1.02765555e-03,  1.73459835e-02, -4.69486136e-03,  4.19088602e-02,  4.50539179e-02,\n",
       "         3.26825306e-02, -3.55810532e-03,  3.85520235e-02],\n",
       "       [ 6.37728393e-01,  9.82435048e-01,  8.75734925e-01,  3.66654426e-01, -3.10895532e-01, -8.45596015e-01, -9.91764188e-01, -6.82239175e-01, -5.92406280e-02,  5.90977609e-01,  7.70261288e-01,  1.86604917e-01, -4.82792199e-01, -9.30357218e-01, -9.50444102e-01, -5.33823371e-01,  1.28077120e-01,\n",
       "         7.31129050e-01,  9.98243749e-01,  8.06687951e-01],\n",
       "       [ 4.48799193e-01,  8.02123189e-01,  9.84807730e-01,  9.57989514e-01,  7.27373660e-01,  3.42020154e-01, -1.16092913e-01, -5.49508989e-01, -8.66025388e-01, -9.98308182e-01,  8.93632650e-01,  5.97158611e-01,  1.73648179e-01, -2.86803246e-01, -6.86241627e-01, -9.39692616e-01, -9.93238330e-01,\n",
       "        -8.35487783e-01, -5.00000000e-01, -5.81448302e-02],\n",
       "       [ 7.27373660e-01,  9.98308182e-01,  6.42787635e-01, -1.16092913e-01, -8.02123189e-01, -9.84807730e-01, -5.49508989e-01,  2.30615869e-01,  8.66025388e-01,  9.57989514e-01,  6.86241627e-01, -5.81448302e-02, -7.66044438e-01, -9.93238330e-01, -5.97158611e-01,  1.73648179e-01,  8.35487783e-01,\n",
       "         9.73044872e-01,  5.00000000e-01, -2.86803246e-01],\n",
       "       [ 7.07106769e-01,  1.00000000e+00,  7.07106769e-01,  1.22464685e-16, -7.07106769e-01, -1.00000000e+00, -7.07106769e-01, -2.44929371e-16,  7.07106769e-01,  1.00000000e+00,  7.07106769e-01,  6.12323426e-17, -7.07106769e-01, -1.00000000e+00, -7.07106769e-01, -1.83697015e-16,  7.07106769e-01,\n",
       "         1.00000000e+00,  7.07106769e-01,  3.06161700e-16],\n",
       "       [-4.78927307e-02,  4.47155759e-02,  2.08253451e-02,  4.61736731e-02,  3.32175121e-02,  1.46236839e-02,  4.90201004e-02, -2.21232064e-02, -3.40165570e-02,  2.11369041e-02,  1.86449997e-02, -2.83230543e-02,  4.85238023e-02, -1.77915338e-02,  4.77470346e-02,  4.17363038e-03, -3.16922851e-02,\n",
       "        -9.88558494e-03, -3.68852727e-02, -1.48976147e-02]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01569275,  0.03015446, -0.03076373, -0.0099005 , -0.02991541, -0.0049784 , -0.01010493,  0.04242973,  0.00039001, -0.04287663, -0.02714549, -0.03437993,  0.01346191,  0.04728324, -0.02907998,  0.04132513,  0.04266026, -0.0353132 , -0.04441606,  0.04734592],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.04267167,  0.01603553, -0.02318999, -0.02544478,  0.01794809,  0.0268517 ,  0.03884757, -0.02883253,  0.0371668 ,  0.03312748, -0.018549  , -0.04372821,  0.02718734,  0.03254878, -0.00393775, -0.03354928, -0.03542963, -0.0124853 ,  0.02571627, -0.01832619],\n",
       "       [ 0.00386665, -0.04247889,  0.03904903,  0.04628646, -0.01371164,  0.00400112,  0.00373326,  0.02738943, -0.04303039,  0.00292228,  0.03434711,  0.01115797,  0.00562338, -0.04661078,  0.04241183, -0.03132064, -0.01496187,  0.01746893, -0.00386314,  0.00705646],\n",
       "       [ 0.00386665, -0.04247889,  0.03904903,  0.04628646, -0.01371164,  0.00400112,  0.00373326,  0.02738943, -0.04303039,  0.00292228,  0.03434711,  0.01115797,  0.00562338, -0.04661078,  0.04241183, -0.03132064, -0.01496187,  0.01746893, -0.00386314,  0.00705646],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.00692248, -0.02313111,  0.02904655, -0.01169006, -0.02382033, -0.04737557, -0.00600157, -0.03598885, -0.00842609, -0.02034912, -0.01187068, -0.01624168, -0.04078236, -0.00624205,  0.00845472, -0.02023822, -0.03820726, -0.01793318, -0.04523567, -0.00379213]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 2600)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim0, dim1, dim2, dim3 = trainX.shape\n",
    "trainX = np.reshape(trainX, (dim0, dim1, dim2 * dim3))#, order='C')\n",
    "trainX = np.reshape(trainX, (dim0, dim1 * dim2 * dim3))#, order='C')\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim0, dim1, dim2 = trainY.shape\n",
    "trainY = np.reshape(trainY, (dim0, dim1 * dim2))#, order='C')\n",
    "trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(dataset.features)\n",
    "cases = np.transpose(features, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 35483, 13)\n",
      "(35483, 13, 10)\n",
      "35483\n",
      "[<AttributeType.CATEGORICAL: 0>, <AttributeType.NUMERICAL: 1>, <AttributeType.CATEGORICAL: 0>, <AttributeType.CATEGORICAL: 0>, <AttributeType.CATEGORICAL: 0>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.NUMERICAL: 1>, <AttributeType.CATEGORICAL: 0>]\n",
      "['name', 'arrival_time', 'company', 'country', 'department', 'global_workload_D', 'global_workload_h', 'local_workload_D', 'local_workload_h', 'user']\n"
     ]
    }
   ],
   "source": [
    "# Debug Prints\n",
    "print(features.shape)\n",
    "print(cases.shape)\n",
    "print(len(dataset.case_lens))\n",
    "print(dataset.attribute_types)\n",
    "print(dataset.event_log.event_attribute_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True  True False False False False  True]\n",
      "(35483, 13, 5)\n"
     ]
    }
   ],
   "source": [
    "# Categorical filter TODO make sure the transformer model can also handle numerical values via multitask learning\n",
    "from utils.enums import AttributeType\n",
    "\n",
    "# Create a boolean mask for categorical attributes\n",
    "categorical_mask = np.array(dataset.attribute_types) == AttributeType.CATEGORICAL\n",
    "print(categorical_mask)\n",
    "num_categorical_features = np.sum(categorical_mask)\n",
    "\n",
    "# Use this mask to filter out only the categorical attributes in the last axis\n",
    "categorical_cases = cases[..., categorical_mask]\n",
    "print(categorical_cases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  2  3 ...  8 10 10]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.case_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(35483, 13, 5)\n",
      "(35483, 5)\n"
     ]
    }
   ],
   "source": [
    "zero_event = np.zeros((num_categorical_features))\n",
    "print(zero_event.shape)\n",
    "\n",
    "trainX_categorical = []\n",
    "trainY_categorical = []\n",
    "for index, (case, case_length) in enumerate(zip(categorical_cases, dataset.case_lens)):\n",
    "    last_event_index = case_length - 1\n",
    "    trainY_categorical.append(case[last_event_index].copy())\n",
    "\n",
    "    # Remove the target event from the training data\n",
    "    case[last_event_index] = zero_event\n",
    "    trainX_categorical.append(case)\n",
    "\n",
    "trainX_categorical = np.array(trainX_categorical, dtype=np.int64)\n",
    "trainY_categorical = np.array(trainY_categorical, dtype=np.int64)\n",
    "\n",
    "print(trainX_categorical.shape)\n",
    "print(trainY_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35483, 65)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim0, dim1, dim2 = trainX_categorical.shape\n",
    "trainX_categorical = np.reshape(trainX_categorical, (dim0, dim1 * dim2))#, order='C')\n",
    "trainX_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   1   1   1   1   5  82  91 113 135   6  83  92 115 137   9  86  98 120 144   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(trainX_categorical[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_categorical[12,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,  82,  91, 113, 135,   6,  83,  92, 115, 137,   9,  86,  98, 120, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_categorical[12,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  82,  91, 113, 135],\n",
       "       [  5,  82,  91, 114, 136],\n",
       "       [  6,  83,  92, 115, 137],\n",
       "       [  6,  83,  92, 116, 138],\n",
       "       [  5,  84,  93, 117, 139],\n",
       "       [  5,  82,  94, 118, 140],\n",
       "       [  5,  84,  93, 117, 139],\n",
       "       [  5,  84,  93, 117, 139],\n",
       "       [  5,  82,  95, 114, 141],\n",
       "       [  7,  85,  96, 119, 142],\n",
       "       [  5,  84,  93, 117, 139],\n",
       "       [  8,  85,  97, 119, 143],\n",
       "       [  2,   2,   2,   2,   2],\n",
       "       [  2,   2,   2,   2,   2],\n",
       "       [  2,   2,   2,   2,   2],\n",
       "       [  2,   2,   2,   2,   2],\n",
       "       [  7,  82,  91, 118, 145],\n",
       "       [  5,  84,  93, 117, 139],\n",
       "       [  6,  83,  95, 115, 146],\n",
       "       [  7,  85,  96, 119, 142]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_categorical[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(trainY_categorical[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 0\n",
    "for encoder in dataset.encoders.values():\n",
    "    largest_index = encoder.largest_attribute()\n",
    "    if largest_index > vocab_size:\n",
    "        vocab_size = largest_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Process Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam # type: ignore\n",
    "from keras.metrics import Mean # type: ignore\n",
    "from tensorflow import data, train, GradientTape, function # type: ignore\n",
    "\n",
    "from novel.transformer.components.transformer import TransformerModel\n",
    "from novel.transformer.components.utils import LRScheduler, loss_fcn, accuracy_fcn, likelihood_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Encoding Sequence Length\n",
      "5 Decoding Sequence Length\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 8\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Instantiate an Adam optimizer\n",
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "# Prepare the training and test splits of the dataset\n",
    "# dataset = PrepareDataset()\n",
    "# trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "# print(f'X Length: {trainX.shape}')\n",
    "enc_seq_length = trainX_categorical.shape[1]\n",
    "dec_seq_length = trainY_categorical.shape[1]\n",
    "# h = 10\n",
    "\n",
    "print(enc_seq_length, \"Encoding Sequence Length\")\n",
    "print(dec_seq_length, \"Decoding Sequence Length\")\n",
    "\n",
    "\n",
    "# d_model = enc_seq_length\n",
    "# d_ff = enc_seq_length\n",
    "\n",
    "# Prepare the dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX_categorical, trainY_categorical))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(\n",
    "    enc_seq_length, \n",
    "    dec_seq_length, \n",
    "    h, d_k, \n",
    "    d_v, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    n, \n",
    "    dropout_rate,\n",
    "    enc_vocab_size=vocab_size,\n",
    "    dec_vocab_size=vocab_size)\n",
    "\n",
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "train_likelihood = Mean(name='train_likelihood')\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of training in 4436 batches\n",
      "(8, 1, 1, 60) Mask\n",
      "(8, 1, 1, 5) Mask\n",
      "(8, 1, 1, 60) Mask\n",
      "(8, 1, 1, 5) Mask\n",
      "Step 0 Loss 5.9620 Accuracy 0.0250 Likelihood 0.0052\n",
      "Step 25 Loss 5.6444 Accuracy 0.0538 Likelihood 0.0101\n",
      "Step 50 Loss 4.9920 Accuracy 0.1593 Likelihood 0.0624\n",
      "Step 75 Loss 4.5551 Accuracy 0.2118 Likelihood 0.1000\n",
      "Step 100 Loss 4.2343 Accuracy 0.2453 Likelihood 0.1419\n",
      "Step 125 Loss 3.9928 Accuracy 0.2694 Likelihood 0.1707\n",
      "Step 150 Loss 3.8106 Accuracy 0.2891 Likelihood 0.1906\n",
      "Step 175 Loss 3.6219 Accuracy 0.3168 Likelihood 0.2160\n",
      "Step 200 Loss 3.4550 Accuracy 0.3389 Likelihood 0.2304\n",
      "Step 225 Loss 3.2693 Accuracy 0.3686 Likelihood 0.2552\n",
      "Step 250 Loss 3.0775 Accuracy 0.3994 Likelihood 0.2839\n",
      "Step 275 Loss 2.9135 Accuracy 0.4255 Likelihood 0.3061\n",
      "Step 300 Loss 2.7583 Accuracy 0.4522 Likelihood 0.3317\n",
      "Step 325 Loss 2.6195 Accuracy 0.4760 Likelihood 0.3540\n",
      "Step 350 Loss 2.4888 Accuracy 0.5004 Likelihood 0.3763\n",
      "Step 375 Loss 2.3635 Accuracy 0.5235 Likelihood 0.4000\n",
      "Step 400 Loss 2.2575 Accuracy 0.5430 Likelihood 0.4194\n",
      "Step 425 Loss 2.1582 Accuracy 0.5612 Likelihood 0.4385\n",
      "Step 450 Loss 2.0622 Accuracy 0.5798 Likelihood 0.4580\n",
      "Step 475 Loss 1.9750 Accuracy 0.5964 Likelihood 0.4763\n",
      "Step 500 Loss 1.8953 Accuracy 0.6120 Likelihood 0.4942\n",
      "Step 525 Loss 1.8246 Accuracy 0.6254 Likelihood 0.5089\n",
      "Step 550 Loss 1.7564 Accuracy 0.6383 Likelihood 0.5240\n",
      "Step 575 Loss 1.6922 Accuracy 0.6514 Likelihood 0.5388\n",
      "Step 600 Loss 1.6337 Accuracy 0.6629 Likelihood 0.5522\n",
      "Step 625 Loss 1.5775 Accuracy 0.6741 Likelihood 0.5655\n",
      "Step 650 Loss 1.5281 Accuracy 0.6834 Likelihood 0.5770\n",
      "Step 675 Loss 1.4804 Accuracy 0.6930 Likelihood 0.5886\n",
      "Step 700 Loss 1.4345 Accuracy 0.7020 Likelihood 0.5996\n",
      "Step 725 Loss 1.3924 Accuracy 0.7102 Likelihood 0.6099\n",
      "Step 750 Loss 1.3526 Accuracy 0.7181 Likelihood 0.6194\n",
      "Step 775 Loss 1.3154 Accuracy 0.7253 Likelihood 0.6286\n",
      "Step 800 Loss 1.2796 Accuracy 0.7324 Likelihood 0.6375\n",
      "Step 825 Loss 1.2482 Accuracy 0.7389 Likelihood 0.6452\n",
      "Step 850 Loss 1.2161 Accuracy 0.7453 Likelihood 0.6533\n",
      "Step 875 Loss 1.1901 Accuracy 0.7504 Likelihood 0.6598\n",
      "Step 900 Loss 1.1613 Accuracy 0.7561 Likelihood 0.6670\n",
      "Step 925 Loss 1.1335 Accuracy 0.7617 Likelihood 0.6742\n",
      "Step 950 Loss 1.1091 Accuracy 0.7665 Likelihood 0.6804\n",
      "Step 975 Loss 1.0849 Accuracy 0.7713 Likelihood 0.6866\n",
      "Step 1000 Loss 1.0618 Accuracy 0.7760 Likelihood 0.6926\n",
      "Step 1025 Loss 1.0407 Accuracy 0.7802 Likelihood 0.6980\n",
      "Step 1050 Loss 1.0202 Accuracy 0.7843 Likelihood 0.7034\n",
      "Step 1075 Loss 1.0006 Accuracy 0.7882 Likelihood 0.7085\n",
      "Step 1100 Loss 0.9835 Accuracy 0.7916 Likelihood 0.7129\n",
      "Step 1125 Loss 0.9647 Accuracy 0.7956 Likelihood 0.7179\n",
      "Step 1150 Loss 0.9468 Accuracy 0.7991 Likelihood 0.7226\n",
      "Step 1175 Loss 0.9316 Accuracy 0.8021 Likelihood 0.7268\n",
      "Step 1200 Loss 0.9165 Accuracy 0.8051 Likelihood 0.7306\n",
      "Step 1225 Loss 0.9008 Accuracy 0.8081 Likelihood 0.7347\n",
      "Step 1250 Loss 0.8861 Accuracy 0.8111 Likelihood 0.7385\n",
      "Step 1275 Loss 0.8711 Accuracy 0.8141 Likelihood 0.7427\n",
      "Step 1300 Loss 0.8576 Accuracy 0.8169 Likelihood 0.7463\n",
      "Step 1325 Loss 0.8460 Accuracy 0.8192 Likelihood 0.7495\n",
      "Step 1350 Loss 0.8334 Accuracy 0.8216 Likelihood 0.7527\n",
      "Step 1375 Loss 0.8215 Accuracy 0.8239 Likelihood 0.7558\n",
      "Step 1400 Loss 0.8107 Accuracy 0.8260 Likelihood 0.7588\n",
      "Step 1425 Loss 0.7989 Accuracy 0.8284 Likelihood 0.7620\n",
      "Step 1450 Loss 0.7884 Accuracy 0.8305 Likelihood 0.7647\n",
      "Step 1475 Loss 0.7787 Accuracy 0.8325 Likelihood 0.7674\n",
      "Step 1500 Loss 0.7689 Accuracy 0.8346 Likelihood 0.7701\n",
      "Step 1525 Loss 0.7587 Accuracy 0.8367 Likelihood 0.7729\n",
      "Step 1550 Loss 0.7500 Accuracy 0.8383 Likelihood 0.7754\n",
      "Step 1575 Loss 0.7433 Accuracy 0.8395 Likelihood 0.7771\n",
      "Step 1600 Loss 0.7348 Accuracy 0.8412 Likelihood 0.7795\n",
      "Step 1625 Loss 0.7275 Accuracy 0.8426 Likelihood 0.7814\n",
      "Step 1650 Loss 0.7210 Accuracy 0.8436 Likelihood 0.7830\n",
      "Step 1675 Loss 0.7136 Accuracy 0.8451 Likelihood 0.7849\n",
      "Step 1700 Loss 0.7055 Accuracy 0.8468 Likelihood 0.7871\n",
      "Step 1725 Loss 0.6971 Accuracy 0.8486 Likelihood 0.7894\n",
      "Step 1750 Loss 0.6895 Accuracy 0.8501 Likelihood 0.7915\n",
      "Step 1775 Loss 0.6823 Accuracy 0.8515 Likelihood 0.7936\n",
      "Step 1800 Loss 0.6751 Accuracy 0.8529 Likelihood 0.7954\n",
      "Step 1825 Loss 0.6701 Accuracy 0.8539 Likelihood 0.7969\n",
      "Step 1850 Loss 0.6637 Accuracy 0.8551 Likelihood 0.7986\n",
      "Step 1875 Loss 0.6588 Accuracy 0.8561 Likelihood 0.7999\n",
      "Step 1900 Loss 0.6527 Accuracy 0.8573 Likelihood 0.8017\n",
      "Step 1925 Loss 0.6471 Accuracy 0.8584 Likelihood 0.8032\n",
      "Step 1950 Loss 0.6407 Accuracy 0.8597 Likelihood 0.8049\n",
      "Step 1975 Loss 0.6345 Accuracy 0.8609 Likelihood 0.8066\n",
      "Step 2000 Loss 0.6290 Accuracy 0.8620 Likelihood 0.8081\n",
      "Step 2025 Loss 0.6238 Accuracy 0.8631 Likelihood 0.8096\n",
      "Step 2050 Loss 0.6194 Accuracy 0.8641 Likelihood 0.8110\n",
      "Step 2075 Loss 0.6152 Accuracy 0.8650 Likelihood 0.8121\n",
      "Step 2100 Loss 0.6120 Accuracy 0.8655 Likelihood 0.8130\n",
      "Step 2125 Loss 0.6074 Accuracy 0.8664 Likelihood 0.8143\n",
      "Step 2150 Loss 0.6020 Accuracy 0.8674 Likelihood 0.8157\n",
      "Step 2175 Loss 0.5973 Accuracy 0.8683 Likelihood 0.8170\n",
      "Step 2200 Loss 0.5922 Accuracy 0.8694 Likelihood 0.8185\n",
      "Step 2225 Loss 0.5876 Accuracy 0.8702 Likelihood 0.8198\n",
      "Step 2250 Loss 0.5836 Accuracy 0.8710 Likelihood 0.8208\n",
      "Step 2275 Loss 0.5790 Accuracy 0.8719 Likelihood 0.8221\n",
      "Step 2300 Loss 0.5744 Accuracy 0.8727 Likelihood 0.8233\n",
      "Step 2325 Loss 0.5709 Accuracy 0.8734 Likelihood 0.8244\n",
      "Step 2350 Loss 0.5674 Accuracy 0.8741 Likelihood 0.8254\n",
      "Step 2375 Loss 0.5656 Accuracy 0.8744 Likelihood 0.8260\n",
      "Step 2400 Loss 0.5613 Accuracy 0.8753 Likelihood 0.8271\n",
      "Step 2425 Loss 0.5568 Accuracy 0.8762 Likelihood 0.8284\n",
      "Step 2450 Loss 0.5536 Accuracy 0.8769 Likelihood 0.8294\n",
      "Step 2475 Loss 0.5517 Accuracy 0.8770 Likelihood 0.8298\n",
      "Step 2500 Loss 0.5480 Accuracy 0.8777 Likelihood 0.8307\n",
      "Step 2525 Loss 0.5441 Accuracy 0.8785 Likelihood 0.8318\n",
      "Step 2550 Loss 0.5399 Accuracy 0.8794 Likelihood 0.8329\n",
      "Step 2575 Loss 0.5365 Accuracy 0.8801 Likelihood 0.8340\n",
      "Step 2600 Loss 0.5332 Accuracy 0.8808 Likelihood 0.8349\n",
      "Step 2625 Loss 0.5300 Accuracy 0.8814 Likelihood 0.8358\n",
      "Step 2650 Loss 0.5275 Accuracy 0.8820 Likelihood 0.8365\n",
      "Step 2675 Loss 0.5245 Accuracy 0.8825 Likelihood 0.8374\n",
      "Step 2700 Loss 0.5216 Accuracy 0.8831 Likelihood 0.8382\n",
      "Step 2725 Loss 0.5188 Accuracy 0.8836 Likelihood 0.8390\n",
      "Step 2750 Loss 0.5161 Accuracy 0.8841 Likelihood 0.8398\n",
      "Step 2775 Loss 0.5139 Accuracy 0.8846 Likelihood 0.8404\n",
      "Step 2800 Loss 0.5105 Accuracy 0.8852 Likelihood 0.8412\n",
      "Step 2825 Loss 0.5082 Accuracy 0.8856 Likelihood 0.8419\n",
      "Step 2850 Loss 0.5062 Accuracy 0.8860 Likelihood 0.8425\n",
      "Step 2875 Loss 0.5038 Accuracy 0.8865 Likelihood 0.8432\n",
      "Step 2900 Loss 0.5015 Accuracy 0.8869 Likelihood 0.8438\n",
      "Step 2925 Loss 0.4991 Accuracy 0.8874 Likelihood 0.8445\n",
      "Step 2950 Loss 0.4964 Accuracy 0.8880 Likelihood 0.8453\n",
      "Step 2975 Loss 0.4936 Accuracy 0.8886 Likelihood 0.8461\n",
      "Step 3000 Loss 0.4921 Accuracy 0.8889 Likelihood 0.8466\n",
      "Step 3025 Loss 0.4901 Accuracy 0.8893 Likelihood 0.8471\n",
      "Step 3050 Loss 0.4875 Accuracy 0.8898 Likelihood 0.8479\n",
      "Step 3075 Loss 0.4848 Accuracy 0.8904 Likelihood 0.8487\n",
      "Step 3100 Loss 0.4829 Accuracy 0.8908 Likelihood 0.8493\n",
      "Step 3125 Loss 0.4811 Accuracy 0.8910 Likelihood 0.8497\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "losses = []\n",
    "\n",
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "        likelihood = likelihood_fcn(decoder_output, prediction)\n",
    "\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "    train_likelihood(likelihood)\n",
    "\n",
    "    return loss, prediction, decoder_output\n",
    "\n",
    "train_loss.reset_states()\n",
    "train_accuracy.reset_states()\n",
    "train_likelihood.reset_states()\n",
    "\n",
    "print(f\"\\nStart of training in {len(train_dataset)} batches\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "    encoder_input = train_batchX[:, num_categorical_features:] #1:] #Skip the start symbol\n",
    "    decoder_input =  train_batchY[:, 0:num_categorical_features]    #train_batchY[:, :-1] \n",
    "    decoder_output = train_batchY[:, :]\n",
    "\n",
    "    # print(encoder_input.shape, \"Encoder Input\")\n",
    "    # print(decoder_input.shape, \"Decoder Input\")\n",
    "    # print(decoder_output.shape, \"Decoder Output\")\n",
    "\n",
    "    loss, prediction, target = train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "    losses.append(loss.numpy())\n",
    "    predictions.append(prediction.numpy())\n",
    "    targets.append(decoder_output.numpy())\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f'Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f} Likelihood {train_likelihood.result():.4f}')\n",
    "\n",
    "# Print epoch number and loss value at the end of every epoch\n",
    "print(\"Training Loss %.4f, Training Accuracy %.4f, Training Likelihood %.4f\" % (train_loss.result(), train_accuracy.result(), train_likelihood.result()))\n",
    "\n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
