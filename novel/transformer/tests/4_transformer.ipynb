{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, cast, float32\n",
    " \n",
    "def padding_mask(input):\n",
    "    # Create mask which marks the zero padding values in the input by a 1\n",
    "    mask = math.equal(input, 0)\n",
    "    mask = cast(mask, float32)\n",
    " \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "input = array([1, 2, 3, 4, 0, 0, 0])\n",
    "print(padding_mask(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import linalg, ones\n",
    "\n",
    "def lookahead_mask(shape):\n",
    "    # Mask out future entries by marking them with a 1.0\n",
    "    mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(lookahead_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from novel.transformer.components.transformer import TransformerModel\n",
    "\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 5  # Maximum length of the input sequence\n",
    "dec_seq_length = 5  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 512)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 5, 512)      131776      ['input_1[0][0]',                \n",
      " HeadAttention)                                                   'input_1[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " add_normalization_30 (AddNorma  (None, 5, 512)      1024        ['input_1[0][0]',                \n",
      " lization)                                                        'dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " feed_forward_12 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_30[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 5, 512)       0           ['feed_forward_12[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_31 (AddNorma  (None, 5, 512)      1024        ['add_normalization_30[0][0]',   \n",
      " lization)                                                        'dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,233,536\n",
      "Trainable params: 2,233,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 5, 512)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 5, 512)      131776      ['input_2[0][0]',                \n",
      " HeadAttention)                                                   'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " add_normalization_32 (AddNorma  (None, 5, 512)      1024        ['input_2[0][0]',                \n",
      " lization)                                                        'dropout_34[0][0]',             \n",
      "                                                                  'add_normalization_32[0][0]',   \n",
      "                                                                  'dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 5, 512)      131776      ['add_normalization_32[0][0]',   \n",
      " HeadAttention)                                                   'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_20[0][0]']\n",
      "                                                                                                  \n",
      " feed_forward_13 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_32[1][0]']   \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 5, 512)       0           ['feed_forward_13[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_34 (AddNorma  (None, 5, 512)      1024        ['add_normalization_32[1][0]',   \n",
      " lization)                                                        'dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,365,312\n",
      "Trainable params: 2,365,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from novel.transformer.components.decoder import DecoderLayer\n",
    "from novel.transformer.components.encoder import EncoderLayer\n",
    "\n",
    "encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "encoder.build_graph().summary()\n",
    " \n",
    "decoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "decoder.build_graph().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcvdb-thesis-bpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
